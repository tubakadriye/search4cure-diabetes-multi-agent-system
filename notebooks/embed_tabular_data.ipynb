{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed CSV Files with Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/\n",
    "\n",
    "https://github.com/mongodb-developer/GenAI-Showcase/blob/50535ba52c872ed03a975bf180f01f84696e7cc9/notebooks/agents/agentic_rag_factory_safety_assistant_with_langgraph_langchain_mongodb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Generative AI Embeddings (AI Studio & Gemini API)\n",
    "\n",
    "Connect to Google's generative AI embeddings service using the `GoogleGenerativeAIEmbeddings` class, found in the [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) package.\n",
    "\n",
    "This will help you get started with Google's Generative AI embedding models (like Gemini) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/v0.2/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "import { ItemTable } from \"@theme/FeatureTables\";\n",
    "\n",
    "<ItemTable category=\"text_embedding\" item=\"Google Gemini\" />\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Google Generative AI embedding models you'll need to create a Google Cloud project, enable the Generative Language API, get an API key, and install the `langchain-google-genai` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "To use Google Generative AI models, you must have an API key. You can create one in Google AI Studio. See the [Google documentation](https://ai.google.dev/gemini-api/docs/api-key) for instructions.\n",
    "\n",
    "Once you have a key, set it as an environment variable `GOOGLE_API_KEY`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "#if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # This loads variables from .env into the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (0.8.1)\n",
      "Collecting google-ai-generativelanguage==0.6.9 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.9-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-api-core in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (2.19.2)\n",
      "Requirement already satisfied: google-api-python-client in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (2.145.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (2.34.0)\n",
      "Requirement already satisfied: protobuf in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (4.25.5)\n",
      "Requirement already satisfied: pydantic in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (2.11.5)\n",
      "Requirement already satisfied: tqdm in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.9->google-generativeai) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google-generativeai) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.9->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Downloading google_ai_generativelanguage-0.6.9-py3-none-any.whl (725 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.4/725.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-ai-generativelanguage\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "    Found existing installation: google-ai-generativelanguage 0.6.18\n",
      "    Uninstalling google-ai-generativelanguage-0.6.18:\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.6.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.5 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_EMBEDDING_MODEL = \"models/embedding-001\"#\"models/gemini-embedding-exp-03-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GenerativeModel(model_name=GEMINI_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=GEMINI_EMBEDDING_MODEL,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the root directory of your project to Python path\n",
    "project_root = os.path.abspath(\"../backend\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MongoDB connection established.\n"
     ]
    }
   ],
   "source": [
    "from db.mongodb_client import mongodb_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the MongoDB database, collection and vector search index\n",
    "DB_NAME = \"diabetes_data\"\n",
    "COLLECTION_NAME = \"records_embeddings\"\n",
    "VS_INDEX_NAME = \"tabular_vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodb_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MongoDB collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: MongoClient(host=['ac-t38w2lp-shard-00-01.q4fmjuw.mongodb.net:27017', 'ac-t38w2lp-shard-00-00.q4fmjuw.mongodb.net:27017', 'ac-t38w2lp-shard-00-02.q4fmjuw.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Search4Cure.diabetes', authsource='admin', replicaset='atlas-lunm1g-shard-0', tls=True, server_api=<pymongo.server_api.ServerApi object at 0x30774adb0>)\n",
      "Databases: ['diabetes_data', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Client:\", mongodb_client)\n",
    "print(\"Databases:\", mongodb_client.list_database_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_credentials = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = path_to_credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def combine_all_attributes(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Combine all attributes (optionally excluding some) of a DataFrame row into a single column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - exclude_columns: list of column names to exclude from the combination\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with a new 'combined_info' column\n",
    "    \"\"\"\n",
    "    exclude_columns = exclude_columns or []\n",
    "\n",
    "    def combine_row(row):\n",
    "        combined = []\n",
    "        for attr in row.index:\n",
    "            if attr in exclude_columns:\n",
    "                continue\n",
    "            value = row[attr]\n",
    "            if isinstance(value, (pd.Series, np.ndarray, list)):\n",
    "                # Handle array-like objects\n",
    "                if len(value) > 0 and not pd.isna(value).all():\n",
    "                    combined.append(f\"{attr.capitalize()}: {value!s}\")\n",
    "            elif not pd.isna(value):\n",
    "                combined.append(f\"{attr.capitalize()}: {value!s}\")\n",
    "        return \" \".join(combined)\n",
    "\n",
    "    df[\"combined_info\"] = df.apply(combine_row, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # === Local Files Folder ===\n",
    "LOCAL_FOLDER = \"../data/data_new\" \n",
    "\n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"⏭️ Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"📂 Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    # Now embed starting from chunk N\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):       \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"✅ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"📌 Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"✅ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n✅ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"✅ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported file: .DS_Store\n",
      "⏭️ Skipping diabetic_data.csv (already uploaded)\n",
      "📂 Processing diabetic_data.csv...\n",
      "✅ Inserted metadata for diabetic_data.csv\n",
      "                                       combined_info\n",
      "0  Encounter_id: 2278392 Patient_nbr: 8222157 Rac...\n",
      "1  Encounter_id: 149190 Patient_nbr: 55629189 Rac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [39:38<00:00,  4.76s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr       race  gender      age weight  \\\n",
      "0       2278392      8222157  Caucasian  Female   [0-10)      ?   \n",
      "1        149190     55629189  Caucasian  Female  [10-20)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    1   \n",
      "1                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                 1  ...  No  No   No   No   No      No           No   \n",
      "1                 3  ...  No  No   No   No   No      Ch          Yes   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 2278392 Patient_nbr: 8222157 Rac...   \n",
      "1         >30  Encounter_id: 149190 Patient_nbr: 55629189 Rac...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.019425964, -0.0024473513, -0.09022015, -0.0...  \n",
      "1  [0.019444333, -0.004087155, -0.07224274, 3.688...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 1: 500 documents inserted.\n",
      "                                         combined_info\n",
      "500  Encounter_id: 4255176 Patient_nbr: 2139525 Rac...\n",
      "501  Encounter_id: 4255452 Patient_nbr: 99109602 Ra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [19:05<00:00,  2.29s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
      "0       4255176      2139525        Caucasian  Female  [60-70)      ?   \n",
      "1       4255452     99109602  AfricanAmerican  Female  [60-70)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    7   \n",
      "1                  1                         6                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                10  ...  No  No   No   No   No      No          Yes   \n",
      "1                10  ...  No  No   No   No   No      No           No   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 4255176 Patient_nbr: 2139525 Rac...   \n",
      "1          NO  Encounter_id: 4255452 Patient_nbr: 99109602 Ra...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022118663, 0.0037105205, -0.07894335, -0.00...  \n",
      "1  [0.020221915, 0.0010955177, -0.07303448, -0.01...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 2: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1000  Encounter_id: 7556418 Patient_nbr: 4282317 Rac...\n",
      "1001  Encounter_id: 7564920 Patient_nbr: 94527 Race:...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [16:35<00:00,  1.99s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr       race gender      age weight  \\\n",
      "0       7556418      4282317  Caucasian   Male  [50-60)      ?   \n",
      "1       7564920        94527  Caucasian   Male  [70-80)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  1                         1                    7   \n",
      "1                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                 6  ...  No  No   No   No   No      No          Yes   \n",
      "1                 2  ...  No  No   No   No   No      No          Yes   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 7556418 Patient_nbr: 4282317 Rac...   \n",
      "1         >30  Encounter_id: 7564920 Patient_nbr: 94527 Race:...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.018956222, 0.002368126, -0.0839956, -0.0046...  \n",
      "1  [0.019801792, 0.0050626164, -0.075723045, 0.00...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 3: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1500  Encounter_id: 10214442 Patient_nbr: 12311595 R...\n",
      "1501  Encounter_id: 10220382 Patient_nbr: 1139715 Ra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  86%|████████▌ | 429/500 [2:08:11<21:12, 17.93s/it]    \n"
     ]
    },
    {
     "ename": "DeadlineExceeded",
     "evalue": "504 Deadline Exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeadlineExceeded\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m duplicated_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     duplicated_rows \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     duplicated_data\u001b[38;5;241m.\u001b[39mextend(duplicated_rows)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicated_data:\n",
      "Cell \u001b[0;32mIn[68], line 23\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(input_data, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)  \u001b[38;5;66;03m# 1.5 seconds delay per request\u001b[39;00m\n\u001b[1;32m     22\u001b[0m chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieval_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional, depends on your use case\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m embedding \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#embedding = embedding_model.embed_query(text=chunk)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/generativeai/embedding.py:213\u001b[0m, in \u001b[0;36membed_content\u001b[0;34m(model, content, task_type, title, output_dimensionality, client, request_options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     embedding_request \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mEmbedContentRequest(\n\u001b[1;32m    207\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    208\u001b[0m         content\u001b[38;5;241m=\u001b[39mcontent_types\u001b[38;5;241m.\u001b[39mto_content(content),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m         output_dimensionality\u001b[38;5;241m=\u001b[39moutput_dimensionality,\n\u001b[1;32m    212\u001b[0m     )\n\u001b[0;32m--> 213\u001b[0m     embedding_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     embedding_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(embedding_response)\u001b[38;5;241m.\u001b[39mto_dict(embedding_response)\n\u001b[1;32m    218\u001b[0m     embedding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1305\u001b[0m, in \u001b[0;36mGenerativeServiceClient.embed_content\u001b[0;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1305\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDeadlineExceeded\u001b[0m: 504 Deadline Exceeded"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"⏭️ Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"📂 Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    # Now embed starting from chunk N\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):       \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"✅ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"📌 Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"✅ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n✅ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"✅ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Processing GALLSTONE_dataset-uci_2025.xlsx...\n",
      "❌ Failed to read GALLSTONE_dataset-uci_2025.xlsx: read_excel() got an unexpected keyword argument 'n_rows'\n",
      "⏭️ Skipping berm_hipdata.csv (already uploaded)\n",
      "📂 Processing berm_hipdata.csv...\n",
      "✅ Inserted metadata for berm_hipdata.csv\n",
      "                                       combined_info\n",
      "0  Diabetes_status: T1D Age: 41.7303217 B_cells_p...\n",
      "1  Diabetes_status: T1D Age: 20.62422998 B_cells_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [2:18:10<00:00, 16.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diabetes_Status        Age  B_cells_pct_of_Lymphocytes  \\\n",
      "0             T1D  41.730322                        5.02   \n",
      "1             T1D  41.730322                        5.02   \n",
      "\n",
      "   Transitional_pct_of_B_cells  Naive_pct_of_B_cells  \\\n",
      "0                         5.22                  68.3   \n",
      "1                         5.22                  68.3   \n",
      "\n",
      "   Nonnegclassnegswitched_Memory_pct_of_B_cells  \\\n",
      "0                                          15.7   \n",
      "1                                          15.7   \n",
      "\n",
      "   Classnegswitched_Memory_pct_of_B_cells    _7  MNC_pct_of_Leukocytes  \\\n",
      "0                                    10.7  0.15                   42.2   \n",
      "1                                    10.7  0.15                   42.2   \n",
      "\n",
      "   Granulocyte_pct_of_Leukocytes  ...  _186  _187  _188  _189  _190  _191  \\\n",
      "0                           54.5  ...   6.0   1.8   0.5  2.89  1.74   0.3   \n",
      "1                           54.5  ...   6.0   1.8   0.5  2.89  1.74   0.3   \n",
      "\n",
      "   _192  _193                                      combined_info  \\\n",
      "0  0.09  0.03  Diabetes_status: T1D Age: 41.7303217 B_cells_p...   \n",
      "1  0.09  0.03  Diabetes_status: T1D Age: 41.7303217 B_cells_p...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.006623911, -0.021135405, -0.05064078, -0.0...  \n",
      "1  [0.013151053, -0.007249503, -0.057959203, -0.0...  \n",
      "\n",
      "[2 rows x 196 columns]\n",
      "Inserted batch 1: 1000 documents\n",
      "❌ Error inserting chunk 1: you are over your space quota, using 517 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 517 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}\n",
      "                                         combined_info\n",
      "500  Diabetes_status: FDR Age: 14.4202601 B_cells_p...\n",
      "501  Diabetes_status: FDR Age: 12.91991786 B_cells_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 302/302 [3:30:06<00:00, 41.74s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diabetes_Status       Age  B_cells_pct_of_Lymphocytes  \\\n",
      "0             FDR  14.42026                         9.6   \n",
      "1             FDR  14.42026                         9.6   \n",
      "\n",
      "   Transitional_pct_of_B_cells  Naive_pct_of_B_cells  \\\n",
      "0                         1.56                  70.3   \n",
      "1                         1.56                  70.3   \n",
      "\n",
      "   Nonnegclassnegswitched_Memory_pct_of_B_cells  \\\n",
      "0                                          10.7   \n",
      "1                                          10.7   \n",
      "\n",
      "   Classnegswitched_Memory_pct_of_B_cells     _7  MNC_pct_of_Leukocytes  \\\n",
      "0                                    17.3  0.159                   50.3   \n",
      "1                                    17.3  0.159                   50.3   \n",
      "\n",
      "   Granulocyte_pct_of_Leukocytes  ...  _186  _187  _188  _189  _190  _191  \\\n",
      "0                           39.8  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1                           39.8  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "   _192  _193                                      combined_info  \\\n",
      "0   NaN   NaN  Diabetes_status: FDR Age: 14.4202601 B_cells_p...   \n",
      "1   NaN   NaN  Diabetes_status: FDR Age: 14.4202601 B_cells_p...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.005214986, -0.0022070894, -0.06716208, 0.00...  \n",
      "1  [0.013148727, -0.009122482, -0.059591357, -0.0...  \n",
      "\n",
      "[2 rows x 196 columns]\n",
      "Inserted batch 1: 1000 documents\n",
      "Inserted batch 2: 1000 documents\n",
      "Inserted batch 3: 86 documents\n",
      "📌 Chunk 2: 2086 documents inserted.\n",
      "✅ Finalized metadata for berm_hipdata.csv: 802 rows\n",
      "📂 Processing diabetes.csv...\n",
      "✅ Inserted metadata for diabetes.csv\n",
      "                                       combined_info\n",
      "0  Pregnancies: 6.0 Glucose: 148.0 Bloodpressure:...\n",
      "1  Pregnancies: 1.0 Glucose: 85.0 Bloodpressure: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [17:52<00:00,  2.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \\\n",
      "0                     0.627   50        1   \n",
      "1                     0.351   31        0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Pregnancies: 6.0 Glucose: 148.0 Bloodpressure:...   \n",
      "1  Pregnancies: 1.0 Glucose: 85.0 Bloodpressure: ...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.0048858738, 0.0013947599, -0.0469905, 0.03...  \n",
      "1  [0.0037936354, 0.0011160112, -0.044843256, 0.0...  \n",
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 1: 500 documents inserted.\n",
      "                                         combined_info\n",
      "500  Pregnancies: 2.0 Glucose: 117.0 Bloodpressure:...\n",
      "501  Pregnancies: 3.0 Glucose: 84.0 Bloodpressure: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 268/268 [08:27<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            2      117             90             19       71  25.2   \n",
      "1            3       84             72             32        0  37.2   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \\\n",
      "0                     0.313   21        0   \n",
      "1                     0.267   28        0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Pregnancies: 2.0 Glucose: 117.0 Bloodpressure:...   \n",
      "1  Pregnancies: 3.0 Glucose: 84.0 Bloodpressure: ...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.0038131082, -0.004052424, -0.04017623, 0.0...  \n",
      "1  [0.0046240217, -0.00027125346, -0.044360377, 0...  \n",
      "Inserted batch 1: 268 documents\n",
      "📌 Chunk 2: 268 documents inserted.\n",
      "✅ Finalized metadata for diabetes.csv: 768 rows\n",
      "\n",
      "✅ Ingestion process completed for all 268 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Local Files Folde7r ===\n",
    "LOCAL_FOLDER = \"../data/data_new2\" \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"⏭️ Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"📂 Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, n_rows=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1000\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    #chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    \n",
    "    # Now embed starting from chunk N\n",
    "    #for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):    \n",
    "    for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"✅ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"📌 Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"✅ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n✅ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"✅ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Skipping diabetes_012_health_indicators_BRFSS2015.csv (already uploaded)\n",
      "📂 Processing diabetes_012_health_indicators_BRFSS2015.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 500it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted metadata for diabetes_012_health_indicators_BRFSS2015.csv\n",
      "                                         combined_info\n",
      "500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:10<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  20.0     0.0     0.0   \n",
      "1           2.0     1.0       0.0        1.0  33.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           0.0     1.0  ...      3.0       3.0   \n",
      "1                   1.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       3.0       0.0  0.0   6.0        6.0     8.0   \n",
      "1      10.0       1.0  1.0  12.0        6.0     2.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.020954559, -0.013000866, -0.07458009, 0.000...  \n",
      "1  [0.021339381, -0.019570077, -0.07782391, 0.001...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 501it [15:14, 914.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 1: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "1001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  25.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  27.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0       2.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0   9.0        4.0     7.0   \n",
      "1       0.0       0.0  0.0  12.0        4.0     3.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02623243, -0.010038114, -0.077802695, 0.002...  \n",
      "1  [0.019862829, -0.016088093, -0.08245769, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 502it [30:19, 908.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 2: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "1501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:06<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  38.0     0.0     0.0   \n",
      "1           2.0     1.0       1.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "1                   1.0           1.0     1.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0   8.0        5.0     4.0   \n",
      "1       0.0       1.0  1.0  13.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02323466, -0.01180313, -0.077059835, -0.002...  \n",
      "1  [0.020149186, -0.023070963, -0.07817461, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 503it [45:29, 909.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 3: 500 documents inserted.\n",
      "                                          combined_info\n",
      "2000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "2001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:06<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  24.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  39.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       0.0   \n",
      "1                   0.0           0.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  8.0        4.0     8.0   \n",
      "1       0.0       0.0  0.0  7.0        5.0     1.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.025745455, -0.0100526195, -0.07606417, 0.00...  \n",
      "1  [0.022724079, -0.014071822, -0.07715163, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 504it [1:00:39, 909.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 4: 500 documents inserted.\n",
      "                                          combined_info\n",
      "2500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "2501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        0.0  40.0     1.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  23.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0      30.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0   3.0        4.0     5.0   \n",
      "1       0.0       1.0  1.0  10.0        4.0     6.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.024271738, -0.019028971, -0.077365614, -0.0...  \n",
      "1  [0.020461898, -0.01638552, -0.07490797, 0.0022...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 505it [1:15:44, 908.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 5: 500 documents inserted.\n",
      "                                          combined_info\n",
      "3000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "3001  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:00<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "1           2.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       3.0   \n",
      "1                   1.0           1.0     0.0  ...      4.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  11.0        6.0     8.0   \n",
      "1      20.0       0.0  0.0  11.0        6.0     6.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021432409, -0.013125011, -0.07944139, -0.00...  \n",
      "1  [0.022299005, -0.020816423, -0.075494304, 0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 506it [1:30:49, 906.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 6: 500 documents inserted.\n",
      "                                          combined_info\n",
      "3500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "3501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:07<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  25.0     1.0     0.0   \n",
      "1           0.0     1.0       0.0        1.0  22.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       7.0       0.0  1.0  8.0        6.0     8.0   \n",
      "1       2.0       0.0  1.0  8.0        5.0     3.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021391436, -0.015417598, -0.07517676, 0.000...  \n",
      "1  [0.02419361, -0.010329009, -0.076397695, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 507it [1:46:02, 909.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 7: 500 documents inserted.\n",
      "                                          combined_info\n",
      "4000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n",
      "4001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:09<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       1.0        1.0  27.0     0.0     0.0   \n",
      "1           0.0     0.0       1.0        1.0  37.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      2.0       0.0   \n",
      "1                   1.0           0.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0   3.0        4.0     7.0   \n",
      "1       2.0       0.0  1.0  10.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.024849609, -0.012472721, -0.076638, 3.52983...  \n",
      "1  [0.021583777, -0.014947215, -0.078158066, -0.0...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 508it [2:01:16, 910.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 8: 500 documents inserted.\n",
      "                                          combined_info\n",
      "4500  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "4501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:13<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       0.0        1.0  31.0     1.0     0.0   \n",
      "1           2.0     1.0       0.0        0.0  36.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      5.0      30.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       7.0       0.0  1.0  12.0        6.0     6.0   \n",
      "1      30.0       1.0  0.0   9.0        5.0     2.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021027742, -0.021490235, -0.07777866, -0.00...  \n",
      "1  [0.025439246, -0.018739127, -0.07689765, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 509it [2:16:34, 912.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 9: 500 documents inserted.\n",
      "                                          combined_info\n",
      "5000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "5001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:02<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  30.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  11.0        4.0     6.0   \n",
      "1       0.0       0.0  0.0  10.0        4.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.020579929, -0.014105658, -0.0814815, -0.001...  \n",
      "1  [0.021050557, -0.015296114, -0.076930575, 0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 510it [2:31:40, 910.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 10: 500 documents inserted.\n",
      "                                          combined_info\n",
      "5500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "5501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  18.0     0.0     0.0   \n",
      "1           0.0     0.0       1.0        1.0  32.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       2.0   \n",
      "1                   0.0           1.0     1.0  ...      3.0       1.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0  3.0        6.0     7.0   \n",
      "1       1.0       0.0  0.0  7.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.023904279, -0.009723195, -0.07740682, 0.001...  \n",
      "1  [0.024270702, -0.009409835, -0.07504707, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 511it [2:46:47, 909.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 11: 500 documents inserted.\n",
      "                                          combined_info\n",
      "6000  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "6001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [14:56<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1           0.0     1.0       0.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       2.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       1.0  0.0  10.0        5.0     5.0   \n",
      "1       0.0       0.0  1.0  10.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021072999, -0.022104833, -0.07585906, -0.00...  \n",
      "1  [0.021860268, -0.014979606, -0.07674988, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 512it [3:01:47, 906.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 12: 500 documents inserted.\n",
      "                                          combined_info\n",
      "6500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "6501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [1:30:24<00:00, 10.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  29.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  22.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0      10.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  7.0        5.0     8.0   \n",
      "1       0.0       0.0  0.0  9.0        5.0     5.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022379091, -0.010802044, -0.07558755, 0.003...  \n",
      "1  [0.025000542, -0.012414336, -0.07771204, 0.002...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 513it [4:32:14, 2276.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 13: 500 documents inserted.\n",
      "                                          combined_info\n",
      "7000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "7001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:12<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  26.0     1.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  30.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "1                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  1.0        4.0     8.0   \n",
      "1       1.0       0.0  0.0  5.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02264808, -0.011972407, -0.075907245, 0.000...  \n",
      "1  [0.023667276, -0.012453735, -0.07615579, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 514it [4:47:30, 1865.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 14: 500 documents inserted.\n",
      "                                          combined_info\n",
      "7500  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "7501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [14:52<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       0.0        1.0  36.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  36.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           0.0     1.0  ...      2.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       4.0       0.0  1.0  9.0        6.0     8.0   \n",
      "1       0.0       1.0  1.0  9.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022964716, -0.0180326, -0.07549729, -0.0014...  \n",
      "1  [0.022559827, -0.014871327, -0.07770404, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 515it [5:02:24, 1572.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 15: 500 documents inserted.\n",
      "                                          combined_info\n",
      "8000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n",
      "8001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [31:48<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       1.0        1.0  33.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  23.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  10.0        4.0     8.0   \n",
      "1       0.0       0.0  0.0   4.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.023811907, -0.0103704175, -0.07728882, 0.00...  \n",
      "1  [0.02427085, -0.00991575, -0.07487146, 0.00229...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 516it [5:34:18, 1675.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 16: 500 documents inserted.\n",
      "                                          combined_info\n",
      "8500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "8501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [54:59<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  32.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  23.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      1.0       0.0   \n",
      "1                   1.0           1.0     0.0  ...      2.0      15.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  10.0        6.0     4.0   \n",
      "1       0.0       0.0  1.0  10.0        6.0     5.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.025151163, -0.009721973, -0.07913976, -0.00...  \n",
      "1  [0.02159506, -0.01475258, -0.07843608, 0.00333...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 517it [6:29:19, 2164.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 17: 500 documents inserted.\n",
      "                                          combined_info\n",
      "9000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "9001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [15:00<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  26.0     0.0     0.0   \n",
      "1           0.0     0.0       1.0        1.0  24.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      2.0      15.0   \n",
      "1                   1.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  8.0        6.0     8.0   \n",
      "1       0.0       0.0  1.0  9.0        6.0     7.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021963626, -0.013149153, -0.07634742, 0.001...  \n",
      "1  [0.024372015, -0.016603777, -0.07668463, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 518it [6:44:23, 1785.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 18: 500 documents inserted.\n",
      "                                          combined_info\n",
      "9500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n",
      "9501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 500/500 [14:55<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       1.0        1.0  35.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  28.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      4.0       0.0   \n",
      "1                   0.0           0.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  7.0        5.0     3.0   \n",
      "1       0.0       0.0  0.0  6.0        5.0     5.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.0250642, -0.011924047, -0.07749991, -0.0003...  \n",
      "1  [0.023737352, -0.01159393, -0.07697324, 0.0018...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 519it [6:59:21, 1518.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "📌 Chunk 19: 500 documents inserted.\n",
      "                                           combined_info\n",
      "10000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "10001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 404/500 [12:09<02:53,  1.80s/it]\n",
      "Embedding: 519it [7:11:31, 1362.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m duplicated_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m     duplicated_rows \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     duplicated_data\u001b[38;5;241m.\u001b[39mextend(duplicated_rows)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicated_data:\n",
      "Cell \u001b[0;32mIn[76], line 23\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(input_data, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)  \u001b[38;5;66;03m# 1.5 seconds delay per request\u001b[39;00m\n\u001b[1;32m     22\u001b[0m chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieval_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional, depends on your use case\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# increase timeout in seconds\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m embedding \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#embedding = embedding_model.embed_query(text=chunk)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/generativeai/embedding.py:124\u001b[0m, in \u001b[0;36membed_content\u001b[0;34m(model, content, task_type, title, output_dimensionality, client, request_options)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_content\u001b[39m(\n\u001b[1;32m    114\u001b[0m     model: model_types\u001b[38;5;241m.\u001b[39mBaseModelNameOptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     client: glm\u001b[38;5;241m.\u001b[39mGenerativeServiceClient \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m     request_options: helper_types\u001b[38;5;241m.\u001b[39mRequestOptionsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m text_types\u001b[38;5;241m.\u001b[39mBatchEmbeddingDict: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_content\u001b[39m(\n\u001b[1;32m    125\u001b[0m     model: model_types\u001b[38;5;241m.\u001b[39mBaseModelNameOptions,\n\u001b[1;32m    126\u001b[0m     content: content_types\u001b[38;5;241m.\u001b[39mContentType \u001b[38;5;241m|\u001b[39m Iterable[content_types\u001b[38;5;241m.\u001b[39mContentType],\n\u001b[1;32m    127\u001b[0m     task_type: EmbeddingTaskTypeOptions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m     output_dimensionality: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    130\u001b[0m     client: glm\u001b[38;5;241m.\u001b[39mGenerativeServiceClient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     request_options: helper_types\u001b[38;5;241m.\u001b[39mRequestOptionsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m text_types\u001b[38;5;241m.\u001b[39mEmbeddingDict \u001b[38;5;241m|\u001b[39m text_types\u001b[38;5;241m.\u001b[39mBatchEmbeddingDict:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the API to create embeddings for content passed in.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m        input content.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_types\u001b[38;5;241m.\u001b[39mmake_model_name(model)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# === Local Files Folde7r ===\n",
    "LOCAL_FOLDER = \"../data/arc\" \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"⏭️ Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"📂 Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    \n",
    "    # Now embed starting from chunk N\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):        \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"✅ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"📌 Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"✅ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n✅ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"✅ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "\n",
    "def insert_df_to_mongodb(df, collection,dataset_id,  batch_size=1000):\n",
    "    \"\"\"\n",
    "    Insert a pandas DataFrame into a MongoDB collection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to insert\n",
    "    collection (pymongo.collection.Collection): The MongoDB collection to insert into\n",
    "    batch_size (int): Number of documents to insert in each batch\n",
    "\n",
    "    Returns:\n",
    "    int: Number of documents successfully inserted\n",
    "    \"\"\"\n",
    "    total_inserted = 0\n",
    "\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict(\"records\")\n",
    "\n",
    "    for record in records:\n",
    "        record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # Insert in batches\n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i : i + batch_size]\n",
    "        try:\n",
    "            result = collection.insert_many(batch, ordered=False)\n",
    "            total_inserted += len(result.inserted_ids)\n",
    "            print(\n",
    "                f\"Inserted batch {i//batch_size + 1}: {len(result.inserted_ids)} documents\"\n",
    "            )\n",
    "        except BulkWriteError as bwe:\n",
    "            total_inserted += bwe.details[\"nInserted\"]\n",
    "            print(\n",
    "                f\"Batch {i//batch_size + 1} partially inserted. {bwe.details['nInserted']} inserted, {len(bwe.details['writeErrors'])} failed.\"\n",
    "            )\n",
    "\n",
    "    return total_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataframe_info(df, df_name):\n",
    "    print(f\"\\n{df_name} DataFrame info:\")\n",
    "    print(df.info())\n",
    "    print(f\"\\nFirst few rows of the {df_name} DataFrame:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512      # Good default for embeddings\n",
    "OVERLAP = 50          # Helps with context continuity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):\n",
    "    \"\"\"\n",
    "    Split the text into overlapping chunks based on token count.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk_tokens = tokens[i : i + max_tokens]\n",
    "        chunk = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input_data, model=GEMINI_EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the 'combined_attributes' column and duplicate the row for each chunk\n",
    "    or generate embeddings for a given string.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, str):\n",
    "        text = input_data\n",
    "    else:\n",
    "        text = input_data[\"combined_info\"]\n",
    "\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    # Split text into chunks if it's too long\n",
    "    chunks = chunk_text(text)\n",
    "\n",
    "    # Embed each chunk\n",
    "    chunk_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        time.sleep(1.5)  # 1.5 seconds delay per request\n",
    "        chunk = chunk.replace(\"\\n\", \" \")\n",
    "        response = genai.embed_content(\n",
    "                model=model,\n",
    "                content=chunk,\n",
    "                task_type=\"retrieval_document\",  # optional, depends on your use case\n",
    "                request_options={\"timeout\": 60}  # increase timeout in seconds\n",
    "            )\n",
    "        embedding = response[\"embedding\"]\n",
    "        #embedding = embedding_model.embed_query(text=chunk)\n",
    "        chunk_embeddings.append(embedding)\n",
    "\n",
    "    if isinstance(input_data, str):\n",
    "        # Return list of embeddings for string input\n",
    "        return chunk_embeddings[0]\n",
    "    # Create duplicated rows for each chunk with the respective embedding for row input\n",
    "    duplicated_rows = []\n",
    "    for embedding in chunk_embeddings:\n",
    "        new_row = input_data.copy()\n",
    "        new_row[\"embedding\"] = embedding\n",
    "        duplicated_rows.append(new_row)\n",
    "    return duplicated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vector search index definition\n",
    "vector_search_index_definition = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": True,\n",
    "        \"fields\": {\n",
    "            \"embedding\": {\n",
    "                \"dimensions\": 256,\n",
    "                \"similarity\": \"cosine\",\n",
    "                \"type\": \"knnVector\",\n",
    "            },\n",
    "            \"recordId\": {\"type\": \"string\"},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically create vector search index for both colelctions\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "\n",
    "def setup_vector_search_index_with_filter(\n",
    "    collection, index_definition, index_name=\"vector_index_with_filter\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup a vector search index for a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    index_definition: Dictionary containing the index definition\n",
    "    index_name: Name of the index (default: \"vector_index_with_filter\")\n",
    "    \"\"\"\n",
    "    new_vector_search_index_model = SearchIndexModel(\n",
    "        definition=index_definition,\n",
    "        name=index_name,\n",
    "    )\n",
    "\n",
    "    # Create the new index\n",
    "    try:\n",
    "        result = collection.create_search_index(model=new_vector_search_index_model)\n",
    "        print(f\"Creating index '{index_name}'...\")\n",
    "        # time.sleep(20)  # Sleep for 20 seconds\n",
    "        print(f\"New index '{index_name}' created successfully:\", result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating new vector search index '{index_name}': {e!s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_vector_search_index_with_filter(\n",
    "    collection, vector_search_index_definition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    vector_search_stage = {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index_with_filter\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"embedding\",\n",
    "            \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "            \"limit\": 5,  # Return top 4 matches\n",
    "        }\n",
    "    }\n",
    "\n",
    "    unset_stage = {\n",
    "        \"$unset\": \"embedding\"  # Exclude the 'embedding' field from the results\n",
    "    }\n",
    "\n",
    "    project_stage = {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,  # Exclude the _id field,\n",
    "            \"combined_info\": 1,\n",
    "            \"score\": {\n",
    "                \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pipeline = [vector_search_stage, unset_stage, project_stage]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_search_result(query, collection):\n",
    "    get_knowledge = vector_search(query, collection)\n",
    "    search_results = []\n",
    "    for result in get_knowledge:\n",
    "        search_results.append(\n",
    "            [result.get(\"score\", \"N/A\"), result.get(\"combined_info\", \"N/A\")]\n",
    "        )\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "query = \"Get me a record having high blood sugar\"\n",
    "source_information = get_vector_search_result(query, collection)\n",
    "\n",
    "table_headers = [\"Similarity Score\", \"Combined Information\"]\n",
    "table = tabulate.tabulate(source_information, headers=table_headers, tablefmt=\"grid\")\n",
    "\n",
    "combined_information = f\"\"\"Query: {query}\n",
    "\n",
    "Continue to answer the query by using the Search Results:\n",
    "\n",
    "{table}\n",
    "\"\"\"\n",
    "\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -U langchain langchain_mongodb langgraph langsmith motor langchain_anthropic # langchain-groq\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programatically create search indexes\n",
    "\n",
    "\n",
    "def create_collection_search_index(collection, index_definition, index_name):\n",
    "    \"\"\"\n",
    "    Create a search index for a MongoDB Atlas collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    index_definition: Dictionary defining the index mappings\n",
    "    index_name: String name for the index\n",
    "\n",
    "    Returns:\n",
    "    str: Result of the index creation operation\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        search_index_model = SearchIndexModel(\n",
    "            definition=index_definition, name=index_name\n",
    "        )\n",
    "\n",
    "        result = collection.create_search_index(model=search_index_model)\n",
    "        print(f\"Search index '{index_name}' created successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating search index: {e!s}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_collection_search_indexes(collection):\n",
    "    \"\"\"\n",
    "    Print all search indexes for a given collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    \"\"\"\n",
    "    print(f\"\\nSearch indexes for collection '{collection.name}':\")\n",
    "    for index in collection.list_search_indexes():\n",
    "        print(f\"Index: {index['name']}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_text_index_definition = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": True,\n",
    "        \"fields\": {\n",
    "            \"title\": {\"type\": \"string\"},\n",
    "            \"description\": {\"type\": \"string\"},\n",
    "            \"category\": {\"type\": \"string\"},\n",
    "            \"steps.description\": {\"type\": \"string\"},\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "create_collection_search_index(\n",
    "    collection,\n",
    "    collection_text_index_definition,\n",
    "    \"text_search_index\",\n",
    ")\n",
    "\n",
    "# Print all indexes in the collection\n",
    "print_collection_search_indexes(collection)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ATLAS_VECTOR_SEARCH_INDEX = \"csv_vector_index_with_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Stores Intialisation\n",
    "vector_store_csv_files = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGO_URI,\n",
    "    namespace=DB_NAME + \".\" + SAFETY_PROCEDURES_COLLECTION,\n",
    "    embedding=embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX,\n",
    "    text_key=\"combined_info\",\n",
    ")\n",
    "\n",
    "hybrid_search = MongoDBAtlasHybridSearchRetriever(\n",
    "    vectorstore=vector_store_csv_files,\n",
    "    search_index_name=\"text_search_index\",\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "hybrid_search_result = hybrid_search.get_relevant_documents(query)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_results_to_table(search_results):\n",
    "    \"\"\"\n",
    "    Convert hybrid search results to a formatted markdown table.\n",
    "\n",
    "    Args:\n",
    "    search_results (list): List of Document objects containing search results\n",
    "\n",
    "    Returns:\n",
    "    str: Formatted markdown table of search results\n",
    "    \"\"\"\n",
    "    # Extract relevant information from each result\n",
    "    data = []\n",
    "    for rank, doc in enumerate(search_results, start=1):\n",
    "        metadata = doc.metadata\n",
    "        data.append(\n",
    "            {\n",
    "                \"Rank\": rank,\n",
    "                \"Procedure ID\": metadata[\"procedureId\"],\n",
    "                \"Title\": metadata[\"title\"],\n",
    "                \"Category\": metadata[\"category\"],\n",
    "                \"Vector Score\": round(metadata[\"vector_score\"], 5),\n",
    "                \"Full-text Score\": round(metadata[\"fulltext_score\"], 5),\n",
    "                \"Total Score\": round(metadata[\"score\"], 5),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate markdown table\n",
    "    table = tabulate.tabulate(df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = hybrid_search_results_to_table(hybrid_search_result)\n",
    "print(table)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.retrievers import MongoDBAtlasFullTextSearchRetriever\n",
    "\n",
    "full_text_search = MongoDBAtlasFullTextSearchRetriever(\n",
    "    collection=collection,\n",
    "    search_index_name=\"text_search_index\",\n",
    "    search_field=\"description\",\n",
    "    top_k=5,\n",
    ")\n",
    "full_text_search_result = full_text_search.get_relevant_documents(\"Guidelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_text_search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections.abc import AsyncIterator\n",
    "from contextlib import AbstractContextManager\n",
    "from datetime import datetime, timezone\n",
    "from types import TracebackType\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.base import (\n",
    "    BaseCheckpointSaver,\n",
    "    Checkpoint,\n",
    "    CheckpointMetadata,\n",
    "    CheckpointTuple,\n",
    "    SerializerProtocol,\n",
    ")\n",
    "from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "from typing_extensions import Self\n",
    "\n",
    "class JsonPlusSerializerCompat(JsonPlusSerializer):\n",
    "    def loads(self, data: bytes) -> Any:\n",
    "        if data.startswith(b\"\\x80\") and data.endswith(b\".\"):\n",
    "            return pickle.loads(data)\n",
    "        return super().loads(data)\n",
    "    \n",
    "class MongoDBSaver(AbstractContextManager, BaseCheckpointSaver):\n",
    "    serde = JsonPlusSerializerCompat()\n",
    "\n",
    "    client: AsyncIOMotorClient\n",
    "    db_name: str\n",
    "    collection_name: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AsyncIOMotorClient,\n",
    "        db_name: str,\n",
    "        collection_name: str,\n",
    "        *,\n",
    "        serde: Optional[SerializerProtocol] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(serde=serde)\n",
    "        self.client = client\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.collection = client[db_name][collection_name]\n",
    "\n",
    "    def __enter__(self) -> Self:\n",
    "        return self\n",
    "\n",
    "    def __exit__(\n",
    "        self,\n",
    "        __exc_type: Optional[type[BaseException]],\n",
    "        __exc_value: Optional[BaseException],\n",
    "        __traceback: Optional[TracebackType],\n",
    "    ) -> Optional[bool]:\n",
    "        return True\n",
    "    \n",
    "    async def aget_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            query = {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": config[\"configurable\"][\"thread_ts\"],\n",
    "            }\n",
    "        else:\n",
    "            query = {\"thread_id\": config[\"configurable\"][\"thread_id\"]}\n",
    "\n",
    "        doc = await self.collection.find_one(query, sort=[(\"thread_ts\", -1)])\n",
    "        if doc:\n",
    "            return CheckpointTuple(\n",
    "                config,\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    async def alist(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> AsyncIterator[CheckpointTuple]:\n",
    "        query = {}\n",
    "        if config is not None:\n",
    "            query[\"thread_id\"] = config[\"configurable\"][\"thread_id\"]\n",
    "        if filter:\n",
    "            for key, value in filter.items():\n",
    "                query[f\"metadata.{key}\"] = value\n",
    "        if before is not None:\n",
    "            query[\"thread_ts\"] = {\"$lt\": before[\"configurable\"][\"thread_ts\"]}\n",
    "\n",
    "        cursor = self.collection.find(query).sort(\"thread_ts\", -1)\n",
    "        if limit:\n",
    "            cursor = cursor.limit(limit)\n",
    "\n",
    "        async for doc in cursor:\n",
    "            yield CheckpointTuple(\n",
    "                {\n",
    "                    \"configurable\": {\n",
    "                        \"thread_id\": doc[\"thread_id\"],\n",
    "                        \"thread_ts\": doc[\"thread_ts\"],\n",
    "                    }\n",
    "                },\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    async def aput(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "        new_versions: Optional[dict[str, Union[str, float, int]]],\n",
    "    ) -> RunnableConfig:\n",
    "        doc = {\n",
    "            \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "            \"thread_ts\": checkpoint[\"id\"],\n",
    "            \"checkpoint\": self.serde.dumps(checkpoint),\n",
    "            \"metadata\": self.serde.dumps(metadata),\n",
    "        }\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            doc[\"parent_ts\"] = config[\"configurable\"][\"thread_ts\"]\n",
    "        await self.collection.insert_one(doc)\n",
    "        return {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": checkpoint[\"id\"],\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Implement synchronous methods as well for compatibility\n",
    "    def get_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        raise NotImplementedError(\"Use aget_tuple for asynchronous operations\")\n",
    "\n",
    "    def list(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Use alist for asynchronous operations\")\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "    ) -> RunnableConfig:\n",
    "        raise NotImplementedError(\"Use aput for asynchronous operations\")\n",
    "\n",
    "    async def aput_writes(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        writes: List[Tuple[str, Any]],\n",
    "        task_id: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Asynchronously store intermediate writes linked to a checkpoint.\"\"\"\n",
    "        docs = []\n",
    "        for channel, value in writes:\n",
    "            doc = {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"task_id\": task_id,\n",
    "                \"channel\": channel,\n",
    "                \"value\": self.serde.dumps(value),\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            }\n",
    "            docs.append(doc)\n",
    "\n",
    "        if docs:\n",
    "            await self.collection.insert_many(docs)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def css_files_vector_search_tool(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search on safety procedures.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        k (int, optional): Number of top results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples (Document, score), where Document is a safety procedure\n",
    "              and score is the similarity score (lower is more similar).\n",
    "\n",
    "    Note:\n",
    "        Uses the global vector_store_safety_procedures for the search.\n",
    "    \"\"\"\n",
    "\n",
    "    vector_search_results = vector_store_safety_procedures.similarity_search_with_score(\n",
    "        query=query, k=k\n",
    "    )\n",
    "    return vector_search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def safety_procedures_full_text_search_tool(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a full-text search on safety procedures.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        k (int, optional): Number of top results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: Relevant safety procedure documents matching the query.\n",
    "    \"\"\"\n",
    "\n",
    "    full_text_search = MongoDBAtlasFullTextSearchRetriever(\n",
    "        collection=safety_procedure_collection,\n",
    "        search_index_name=\"text_search_index\",\n",
    "        search_field=\"description\",\n",
    "        top_k=k,\n",
    "    )\n",
    "\n",
    "    full_text_search_result = full_text_search.get_relevant_documents(query)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def safety_procedures_hybrid_search_tool(query: str):\n",
    "    \"\"\"\n",
    "    Perform a hybrid (vector + full-text) search on safety procedures.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        list: Relevant safety procedure documents from hybrid search.\n",
    "\n",
    "    Note:\n",
    "        Uses both vector_store_safety_procedures and text_search_index.\n",
    "    \"\"\"\n",
    "\n",
    "    hybrid_search = MongoDBAtlasHybridSearchRetriever(\n",
    "        vectorstore=vector_store_safety_procedures,\n",
    "        search_index_name=\"text_search_index\",\n",
    "        top_k=5,\n",
    "    )\n",
    "\n",
    "    hybrid_search_result = hybrid_search.get_relevant_documents(query)\n",
    "\n",
    "    return hybrid_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    stepNumber: int = Field(..., ge=1)\n",
    "    description: str\n",
    "\n",
    "\n",
    "class SafetyProcedure(BaseModel):\n",
    "    procedureId: str\n",
    "    title: str\n",
    "    description: str\n",
    "    category: str\n",
    "    steps: List[Step]\n",
    "    lastUpdated: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "def create_safety_procedure_document(procedure_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Create a new safety procedure document from a dictionary, using Pydantic for validation.\n",
    "\n",
    "    Args:\n",
    "    procedure_data (dict): Dictionary representing the new safety procedure\n",
    "\n",
    "    Returns:\n",
    "    dict: Validated and formatted safety procedure document\n",
    "\n",
    "    Raises:\n",
    "    ValidationError: If the input data doesn't match the SafetyProcedure schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a SafetyProcedure instance, which will validate the data\n",
    "        safety_procedure = SafetyProcedure(**procedure_data)\n",
    "\n",
    "        # Convert the Pydantic model to a dictionary\n",
    "        document = safety_procedure.dict()\n",
    "\n",
    "        # Ensure steps are properly numbered\n",
    "        for i, step in enumerate(document[\"steps\"], start=1):\n",
    "            step[\"stepNumber\"] = i\n",
    "\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid safety procedure data: {e!s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to add new safety procedures\n",
    "@tool\n",
    "def create_new_safety_procedures(new_procedure: dict):\n",
    "    \"\"\"\n",
    "    Create and validate a new safety procedure document.\n",
    "\n",
    "    Args:\n",
    "        new_procedure (dict): Dictionary containing the new safety procedure data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validated and formatted safety procedure document.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input data is invalid or doesn't match the required schema.\n",
    "\n",
    "    Note:\n",
    "        Uses Pydantic for data validation via create_safety_procedure_document function.\n",
    "    \"\"\"\n",
    "    new_safety_procedure_document = create_safety_procedure_document(new_procedure)\n",
    "    return new_safety_procedure_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_procedure_collection_tools = [\n",
    "    safety_procedures_vector_search_tool,\n",
    "    safety_procedures_full_text_search_tool,\n",
    "    safety_procedures_hybrid_search_tool,\n",
    "    create_new_safety_procedures,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\"\n",
    "                \"\\nCurrent time: {time}.\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(time=lambda: str(datetime.now()))\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "\n",
    "    return prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot agent and node\n",
    "toolbox = []\n",
    "\n",
    "# Add tools\n",
    "toolbox.extend(safety_procedure_collection_tools)\n",
    "toolbox.extend(accident_report_collection_tools)\n",
    "\n",
    "# Create Agent\n",
    "chatbot_agent = create_agent(\n",
    "    llm,\n",
    "    toolbox,\n",
    "    system_message=\"\"\"\n",
    "      You are an advanced Factory Safety Assistant Agent specializing in managing and providing information about safety procedures and accident reports in industrial settings. Your key responsibilities include:\n",
    "\n",
    "      1. Searching and retrieving safety procedures and accident reports:\n",
    "        - Use the provided search tools to find relevant safety procedures and accident reports based on user queries\n",
    "        - Interpret and explain safety procedures and accident reports in detail\n",
    "        - Provide context and additional information related to specific safety protocols and past incidents\n",
    "\n",
    "      2. Creating new safety procedures and accident reports:\n",
    "        - When provided with appropriate information, use the create_new_safety_procedures tool to generate new safety procedure documents\n",
    "        - Use the create_new_accident_report tool to document new accidents or incidents\n",
    "        - Ensure all necessary details are included in new procedures and reports\n",
    "\n",
    "      3. Answering safety-related queries:\n",
    "        - Respond to questions about safety protocols, best practices, regulations, and past incidents\n",
    "        - Offer explanations and clarifications on complex safety issues\n",
    "        - Provide step-by-step guidance on implementing safety procedures and handling incidents\n",
    "\n",
    "      4. Assisting with safety compliance and incident prevention:\n",
    "        - Help identify relevant safety procedures for specific tasks or situations\n",
    "        - Advise on how to adhere to safety guidelines and regulations\n",
    "        - Suggest improvements or updates to existing safety procedures based on past incidents\n",
    "        - Analyze accident reports to identify trends and recommend preventive measures\n",
    "\n",
    "      5. Supporting safety training and awareness:\n",
    "        - Explain the importance and rationale behind safety procedures\n",
    "        - Offer tips and best practices for maintaining a safe work environment\n",
    "        - Help users understand the potential risks and consequences of not following safety procedures\n",
    "        - Use past incident reports to illustrate the importance of safety measures\n",
    "\n",
    "        6. Providing Structured Safety Advice:\n",
    "   When users ask for safety procedures advice, provide information in the following structured format:\n",
    "\n",
    "   Safety Procedure Advice:\n",
    "   a. Relevant Procedure:\n",
    "      - Title: [Procedure Title]\n",
    "      - ID: [Procedure ID]\n",
    "      - Description: [Brief description of the procedure]\n",
    "      - Key Steps:\n",
    "        1. [Step 1]\n",
    "        2. [Step 2]\n",
    "        3. [...]\n",
    "\n",
    "   b. Related Incidents (Past 2 Years):\n",
    "      - Incident 1:\n",
    "        - IncidentID: [ID of the Incident document]\n",
    "        - Date: [Date of incident]\n",
    "        - Description: [Brief description of the incident]\n",
    "        - Root Cause(s): [Identified root cause(s)]\n",
    "      - Incident 2:\n",
    "        - [Same structure as Incident 1]\n",
    "      - [Additional incidents if applicable]\n",
    "\n",
    "   c. Possible Root Causes:\n",
    "      - [List of potential root causes based on the procedure and related incidents]\n",
    "\n",
    "   d. Additional Safety Recommendations:\n",
    "      - [Any extra safety tips or precautions based on the procedure and incident history]\n",
    "\n",
    "   e. References:\n",
    "      - Safety Procedure: [Reference to the specific safety procedure document]\n",
    "      - Incident Reports: [References to the relevant incident reports]\n",
    "\n",
    "When providing this structured advice:\n",
    "- Use the safety procedure search tools to find the most relevant procedure.\n",
    "- Utilize the accident report search tools to identify related incidents from the past two years in the same region.\n",
    "- Analyze the incident reports to identify common or significant root causes.\n",
    "- Provide additional recommendations based on your analysis of both the procedure and the incident history.\n",
    "- Always include clear references to the source documents for both procedures and incident reports.\n",
    "\n",
    "\n",
    "      When creating a new safety procedure, ensure you have all required information and use the create_new_safety_procedures tool. The required fields are:\n",
    "      - procedureId\n",
    "      - title\n",
    "      - description\n",
    "      - category\n",
    "      - steps (a list of step objects, each with a stepNumber and description)\n",
    "\n",
    "      When creating a new accident report, use the create_new_accident_report tool. Ensure you gather all necessary information about the incident.\n",
    "\n",
    "      Provide detailed, accurate, and helpful information to support factory workers, managers, and safety officers in maintaining a safe work environment and properly documenting incidents. If you cannot find specific information or if the information requested is not available, clearly state this and offer to assist in creating a new procedure or report if appropriate.\n",
    "\n",
    "      When discussing safety matters, always prioritize the well-being of workers and adherence to safety regulations. Use information from accident reports to reinforce the importance of following safety procedures and to suggest improvements in safety protocols.\n",
    "\n",
    "      DO NOT MAKE UP ANY INFORMATION.\n",
    "    \"\"\",\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "chatbot_node = functools.partial(\n",
    "    agent_node, agent=chatbot_agent, name=\"Diabetes Research Assistant Agent( DRAA)\"\n",
    ")\n",
    "tool_node = ToolNode(toolbox, name=\"tools\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflow Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"chatbot\")\n",
    "workflow.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\", END: END})\n",
    "\n",
    "workflow.add_edge(\"tools\", \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "mongo_client = AsyncIOMotorClient(MONGO_URI)\n",
    "mongodb_checkpointer = MongoDBSaver(mongo_client, DB_NAME, \"state_store\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=mongodb_checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"Sanitize the name to match the pattern '^[a-zA-Z0-9_-]+$'.\"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "async def chat_loop():\n",
    "    config = {\"configurable\": {\"thread_id\": \"0\"}}\n",
    "\n",
    "    while True:\n",
    "        user_input = await asyncio.get_event_loop().run_in_executor(\n",
    "            None, input, \"User: \"\n",
    "        )\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        sanitized_name = (\n",
    "            sanitize_name(\"Human\") or \"Anonymous\"\n",
    "        )  # Fallback if sanitized name is empty\n",
    "        state = {\"messages\": [HumanMessage(content=user_input, name=sanitized_name)]}\n",
    "\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        max_retries = 3\n",
    "        retry_delay = 1\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async for chunk in graph.astream(state, config, stream_mode=\"values\"):\n",
    "                    if chunk.get(\"messages\"):\n",
    "                        last_message = chunk[\"messages\"][-1]\n",
    "                        if isinstance(last_message, AIMessage):\n",
    "                            last_message.name = (\n",
    "                                sanitize_name(last_message.name or \"AI\") or \"AI\"\n",
    "                            )\n",
    "                            print(last_message.content, end=\"\", flush=True)\n",
    "                    elif isinstance(last_message, ToolMessage):\n",
    "                        print(f\"\\n[Tool Used: {last_message.name}]\")\n",
    "                        print(f\"Tool Call ID: {last_message.tool_call_id}\")\n",
    "                        print(f\"Content: {last_message.content}\")\n",
    "                        print(\"Assistant: \", end=\"\", flush=True)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"\\nAn unexpected error occurred: {e!s}\")\n",
    "                    print(f\"\\nRetrying in {retry_delay} seconds...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                else:\n",
    "                    print(f\"\\nMax retries reached. OpenAI API error: {e!s}\")\n",
    "                    break\n",
    "\n",
    "        print(\"\\n\")  # New line after the complete response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Jupyter notebooks and IPython environments\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "await chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
