{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed CSV Files with Google Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/\n",
    "\n",
    "https://github.com/mongodb-developer/GenAI-Showcase/blob/50535ba52c872ed03a975bf180f01f84696e7cc9/notebooks/agents/agentic_rag_factory_safety_assistant_with_langgraph_langchain_mongodb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Generative AI Embeddings (AI Studio & Gemini API)\n",
    "\n",
    "Connect to Google's generative AI embeddings service using the `GoogleGenerativeAIEmbeddings` class, found in the [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) package.\n",
    "\n",
    "This will help you get started with Google's Generative AI embedding models (like Gemini) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/v0.2/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "import { ItemTable } from \"@theme/FeatureTables\";\n",
    "\n",
    "<ItemTable category=\"text_embedding\" item=\"Google Gemini\" />\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Google Generative AI embedding models you'll need to create a Google Cloud project, enable the Generative Language API, get an API key, and install the `langchain-google-genai` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "To use Google Generative AI models, you must have an API key. You can create one in Google AI Studio. See the [Google documentation](https://ai.google.dev/gemini-api/docs/api-key) for instructions.\n",
    "\n",
    "Once you have a key, set it as an environment variable `GOOGLE_API_KEY`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "#if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # This loads variables from .env into the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall -y google-generativeai google-ai-generativelanguage==0.6.18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install google-generativeai==0.3.2 google-ai-generativelanguage==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_EMBEDDING_MODEL = \"models/embedding-001\"#\"models/gemini-embedding-exp-03-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.generativeai import GenerativeModel\n",
    "#GEMINI_EMBEDDING_MODEL = \"models/embedding-001\"#\"models/gemini-embedding-exp-03-07\"\n",
    "#embedding_model = GenerativeModel(model_name=GEMINI_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=GEMINI_EMBEDDING_MODEL,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GenerativeModel(model_name=GEMINI_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the root directory of your project to Python path\n",
    "project_root = os.path.abspath(\"../backend\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MongoDB connection established.\n"
     ]
    }
   ],
   "source": [
    "from db.mongodb_client import mongodb_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the MongoDB database, collection and vector search index\n",
    "DB_NAME = \"diabetes_data\"\n",
    "COLLECTION_NAME = \"records_embeddings\"\n",
    "VS_INDEX_NAME = \"csv_vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongodb_client[DB_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MongoDB collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: MongoClient(host=['ac-t38w2lp-shard-00-02.q4fmjuw.mongodb.net:27017', 'ac-t38w2lp-shard-00-01.q4fmjuw.mongodb.net:27017', 'ac-t38w2lp-shard-00-00.q4fmjuw.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Search4Cure.diabetes', authsource='admin', replicaset='atlas-lunm1g-shard-0', tls=True, server_api=<pymongo.server_api.ServerApi object at 0x16ba93c50>)\n",
      "Databases: ['diabetes_data', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Client:\", mongodb_client)\n",
    "print(\"Databases:\", mongodb_client.list_database_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_credentials = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = path_to_credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def combine_all_attributes(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Combine all attributes (optionally excluding some) of a DataFrame row into a single column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - exclude_columns: list of column names to exclude from the combination\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with a new 'combined_info' column\n",
    "    \"\"\"\n",
    "    exclude_columns = exclude_columns or []\n",
    "\n",
    "    def combine_row(row):\n",
    "        combined = []\n",
    "        for attr in row.index:\n",
    "            if attr in exclude_columns:\n",
    "                continue\n",
    "            value = row[attr]\n",
    "            if isinstance(value, (pd.Series, np.ndarray, list)):\n",
    "                # Handle array-like objects\n",
    "                if len(value) > 0 and not pd.isna(value).all():\n",
    "                    combined.append(f\"{attr.capitalize()}: {value!s}\")\n",
    "            elif not pd.isna(value):\n",
    "                combined.append(f\"{attr.capitalize()}: {value!s}\")\n",
    "        return \" \".join(combined)\n",
    "\n",
    "    df[\"combined_info\"] = df.apply(combine_row, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # === Local Files Folder ===\n",
    "LOCAL_FOLDER = \"../data/data_new\" \n",
    "\n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"‚è≠Ô∏è Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"üìÇ Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    # Now embed starting from chunk N\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):       \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"‚úÖ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"üìå Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"‚úÖ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n‚úÖ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"‚úÖ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported file: .DS_Store\n",
      "‚è≠Ô∏è Skipping diabetic_data.csv (already uploaded)\n",
      "üìÇ Processing diabetic_data.csv...\n",
      "‚úÖ Inserted metadata for diabetic_data.csv\n",
      "                                       combined_info\n",
      "0  Encounter_id: 2278392 Patient_nbr: 8222157 Rac...\n",
      "1  Encounter_id: 149190 Patient_nbr: 55629189 Rac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [39:38<00:00,  4.76s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr       race  gender      age weight  \\\n",
      "0       2278392      8222157  Caucasian  Female   [0-10)      ?   \n",
      "1        149190     55629189  Caucasian  Female  [10-20)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    1   \n",
      "1                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                 1  ...  No  No   No   No   No      No           No   \n",
      "1                 3  ...  No  No   No   No   No      Ch          Yes   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 2278392 Patient_nbr: 8222157 Rac...   \n",
      "1         >30  Encounter_id: 149190 Patient_nbr: 55629189 Rac...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.019425964, -0.0024473513, -0.09022015, -0.0...  \n",
      "1  [0.019444333, -0.004087155, -0.07224274, 3.688...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 1: 500 documents inserted.\n",
      "                                         combined_info\n",
      "500  Encounter_id: 4255176 Patient_nbr: 2139525 Rac...\n",
      "501  Encounter_id: 4255452 Patient_nbr: 99109602 Ra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [19:05<00:00,  2.29s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
      "0       4255176      2139525        Caucasian  Female  [60-70)      ?   \n",
      "1       4255452     99109602  AfricanAmerican  Female  [60-70)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    7   \n",
      "1                  1                         6                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                10  ...  No  No   No   No   No      No          Yes   \n",
      "1                10  ...  No  No   No   No   No      No           No   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 4255176 Patient_nbr: 2139525 Rac...   \n",
      "1          NO  Encounter_id: 4255452 Patient_nbr: 99109602 Ra...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022118663, 0.0037105205, -0.07894335, -0.00...  \n",
      "1  [0.020221915, 0.0010955177, -0.07303448, -0.01...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 2: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1000  Encounter_id: 7556418 Patient_nbr: 4282317 Rac...\n",
      "1001  Encounter_id: 7564920 Patient_nbr: 94527 Race:...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [16:35<00:00,  1.99s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr       race gender      age weight  \\\n",
      "0       7556418      4282317  Caucasian   Male  [50-60)      ?   \n",
      "1       7564920        94527  Caucasian   Male  [70-80)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  1                         1                    7   \n",
      "1                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... _42 _43  _44  _45  _46  change  diabetesMed  \\\n",
      "0                 6  ...  No  No   No   No   No      No          Yes   \n",
      "1                 2  ...  No  No   No   No   No      No          Yes   \n",
      "\n",
      "   readmitted                                      combined_info  \\\n",
      "0          NO  Encounter_id: 7556418 Patient_nbr: 4282317 Rac...   \n",
      "1         >30  Encounter_id: 7564920 Patient_nbr: 94527 Race:...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.018956222, 0.002368126, -0.0839956, -0.0046...  \n",
      "1  [0.019801792, 0.0050626164, -0.075723045, 0.00...  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 3: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1500  Encounter_id: 10214442 Patient_nbr: 12311595 R...\n",
      "1501  Encounter_id: 10220382 Patient_nbr: 1139715 Ra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 429/500 [2:08:11<21:12, 17.93s/it]    \n"
     ]
    },
    {
     "ename": "DeadlineExceeded",
     "evalue": "504 Deadline Exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeadlineExceeded\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m duplicated_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     duplicated_rows \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     duplicated_data\u001b[38;5;241m.\u001b[39mextend(duplicated_rows)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicated_data:\n",
      "Cell \u001b[0;32mIn[68], line 23\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(input_data, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)  \u001b[38;5;66;03m# 1.5 seconds delay per request\u001b[39;00m\n\u001b[1;32m     22\u001b[0m chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieval_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional, depends on your use case\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m embedding \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#embedding = embedding_model.embed_query(text=chunk)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/generativeai/embedding.py:213\u001b[0m, in \u001b[0;36membed_content\u001b[0;34m(model, content, task_type, title, output_dimensionality, client, request_options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     embedding_request \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mEmbedContentRequest(\n\u001b[1;32m    207\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    208\u001b[0m         content\u001b[38;5;241m=\u001b[39mcontent_types\u001b[38;5;241m.\u001b[39mto_content(content),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m         output_dimensionality\u001b[38;5;241m=\u001b[39moutput_dimensionality,\n\u001b[1;32m    212\u001b[0m     )\n\u001b[0;32m--> 213\u001b[0m     embedding_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     embedding_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(embedding_response)\u001b[38;5;241m.\u001b[39mto_dict(embedding_response)\n\u001b[1;32m    218\u001b[0m     embedding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1305\u001b[0m, in \u001b[0;36mGenerativeServiceClient.embed_content\u001b[0;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1305\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDeadlineExceeded\u001b[0m: 504 Deadline Exceeded"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"‚è≠Ô∏è Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"üìÇ Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    # Now embed starting from chunk N\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):       \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"‚úÖ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"üìå Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"‚úÖ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n‚úÖ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"‚úÖ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing GALLSTONE_dataset-uci_2025.xlsx...\n",
      "‚ùå Failed to read GALLSTONE_dataset-uci_2025.xlsx: read_excel() got an unexpected keyword argument 'n_rows'\n",
      "‚è≠Ô∏è Skipping berm_hipdata.csv (already uploaded)\n",
      "üìÇ Processing berm_hipdata.csv...\n",
      "‚úÖ Inserted metadata for berm_hipdata.csv\n",
      "                                       combined_info\n",
      "0  Diabetes_status: T1D Age: 41.7303217 B_cells_p...\n",
      "1  Diabetes_status: T1D Age: 20.62422998 B_cells_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [2:18:10<00:00, 16.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diabetes_Status        Age  B_cells_pct_of_Lymphocytes  \\\n",
      "0             T1D  41.730322                        5.02   \n",
      "1             T1D  41.730322                        5.02   \n",
      "\n",
      "   Transitional_pct_of_B_cells  Naive_pct_of_B_cells  \\\n",
      "0                         5.22                  68.3   \n",
      "1                         5.22                  68.3   \n",
      "\n",
      "   Nonnegclassnegswitched_Memory_pct_of_B_cells  \\\n",
      "0                                          15.7   \n",
      "1                                          15.7   \n",
      "\n",
      "   Classnegswitched_Memory_pct_of_B_cells    _7  MNC_pct_of_Leukocytes  \\\n",
      "0                                    10.7  0.15                   42.2   \n",
      "1                                    10.7  0.15                   42.2   \n",
      "\n",
      "   Granulocyte_pct_of_Leukocytes  ...  _186  _187  _188  _189  _190  _191  \\\n",
      "0                           54.5  ...   6.0   1.8   0.5  2.89  1.74   0.3   \n",
      "1                           54.5  ...   6.0   1.8   0.5  2.89  1.74   0.3   \n",
      "\n",
      "   _192  _193                                      combined_info  \\\n",
      "0  0.09  0.03  Diabetes_status: T1D Age: 41.7303217 B_cells_p...   \n",
      "1  0.09  0.03  Diabetes_status: T1D Age: 41.7303217 B_cells_p...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.006623911, -0.021135405, -0.05064078, -0.0...  \n",
      "1  [0.013151053, -0.007249503, -0.057959203, -0.0...  \n",
      "\n",
      "[2 rows x 196 columns]\n",
      "Inserted batch 1: 1000 documents\n",
      "‚ùå Error inserting chunk 1: you are over your space quota, using 517 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 517 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}\n",
      "                                         combined_info\n",
      "500  Diabetes_status: FDR Age: 14.4202601 B_cells_p...\n",
      "501  Diabetes_status: FDR Age: 12.91991786 B_cells_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 302/302 [3:30:06<00:00, 41.74s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diabetes_Status       Age  B_cells_pct_of_Lymphocytes  \\\n",
      "0             FDR  14.42026                         9.6   \n",
      "1             FDR  14.42026                         9.6   \n",
      "\n",
      "   Transitional_pct_of_B_cells  Naive_pct_of_B_cells  \\\n",
      "0                         1.56                  70.3   \n",
      "1                         1.56                  70.3   \n",
      "\n",
      "   Nonnegclassnegswitched_Memory_pct_of_B_cells  \\\n",
      "0                                          10.7   \n",
      "1                                          10.7   \n",
      "\n",
      "   Classnegswitched_Memory_pct_of_B_cells     _7  MNC_pct_of_Leukocytes  \\\n",
      "0                                    17.3  0.159                   50.3   \n",
      "1                                    17.3  0.159                   50.3   \n",
      "\n",
      "   Granulocyte_pct_of_Leukocytes  ...  _186  _187  _188  _189  _190  _191  \\\n",
      "0                           39.8  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1                           39.8  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "   _192  _193                                      combined_info  \\\n",
      "0   NaN   NaN  Diabetes_status: FDR Age: 14.4202601 B_cells_p...   \n",
      "1   NaN   NaN  Diabetes_status: FDR Age: 14.4202601 B_cells_p...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.005214986, -0.0022070894, -0.06716208, 0.00...  \n",
      "1  [0.013148727, -0.009122482, -0.059591357, -0.0...  \n",
      "\n",
      "[2 rows x 196 columns]\n",
      "Inserted batch 1: 1000 documents\n",
      "Inserted batch 2: 1000 documents\n",
      "Inserted batch 3: 86 documents\n",
      "üìå Chunk 2: 2086 documents inserted.\n",
      "‚úÖ Finalized metadata for berm_hipdata.csv: 802 rows\n",
      "üìÇ Processing diabetes.csv...\n",
      "‚úÖ Inserted metadata for diabetes.csv\n",
      "                                       combined_info\n",
      "0  Pregnancies: 6.0 Glucose: 148.0 Bloodpressure:...\n",
      "1  Pregnancies: 1.0 Glucose: 85.0 Bloodpressure: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [17:52<00:00,  2.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \\\n",
      "0                     0.627   50        1   \n",
      "1                     0.351   31        0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Pregnancies: 6.0 Glucose: 148.0 Bloodpressure:...   \n",
      "1  Pregnancies: 1.0 Glucose: 85.0 Bloodpressure: ...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.0048858738, 0.0013947599, -0.0469905, 0.03...  \n",
      "1  [0.0037936354, 0.0011160112, -0.044843256, 0.0...  \n",
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 1: 500 documents inserted.\n",
      "                                         combined_info\n",
      "500  Pregnancies: 2.0 Glucose: 117.0 Bloodpressure:...\n",
      "501  Pregnancies: 3.0 Glucose: 84.0 Bloodpressure: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268/268 [08:27<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            2      117             90             19       71  25.2   \n",
      "1            3       84             72             32        0  37.2   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \\\n",
      "0                     0.313   21        0   \n",
      "1                     0.267   28        0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Pregnancies: 2.0 Glucose: 117.0 Bloodpressure:...   \n",
      "1  Pregnancies: 3.0 Glucose: 84.0 Bloodpressure: ...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.0038131082, -0.004052424, -0.04017623, 0.0...  \n",
      "1  [0.0046240217, -0.00027125346, -0.044360377, 0...  \n",
      "Inserted batch 1: 268 documents\n",
      "üìå Chunk 2: 268 documents inserted.\n",
      "‚úÖ Finalized metadata for diabetes.csv: 768 rows\n",
      "\n",
      "‚úÖ Ingestion process completed for all 268 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Local Files Folde7r ===\n",
    "LOCAL_FOLDER = \"../data/data_new2\" \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"‚è≠Ô∏è Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"üìÇ Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, n_rows=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 1000\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    #chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    \n",
    "    # Now embed starting from chunk N\n",
    "    #for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):    \n",
    "    for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"‚úÖ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"üìå Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"‚úÖ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n‚úÖ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"‚úÖ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping diabetes_012_health_indicators_BRFSS2015.csv (already uploaded)\n",
      "üìÇ Processing diabetes_012_health_indicators_BRFSS2015.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 500it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted metadata for diabetes_012_health_indicators_BRFSS2015.csv\n",
      "                                         combined_info\n",
      "500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:10<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  20.0     0.0     0.0   \n",
      "1           2.0     1.0       0.0        1.0  33.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           0.0     1.0  ...      3.0       3.0   \n",
      "1                   1.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       3.0       0.0  0.0   6.0        6.0     8.0   \n",
      "1      10.0       1.0  1.0  12.0        6.0     2.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.020954559, -0.013000866, -0.07458009, 0.000...  \n",
      "1  [0.021339381, -0.019570077, -0.07782391, 0.001...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 501it [15:14, 914.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 1: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "1001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  25.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  27.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0       2.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0   9.0        4.0     7.0   \n",
      "1       0.0       0.0  0.0  12.0        4.0     3.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02623243, -0.010038114, -0.077802695, 0.002...  \n",
      "1  [0.019862829, -0.016088093, -0.08245769, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 502it [30:19, 908.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 2: 500 documents inserted.\n",
      "                                          combined_info\n",
      "1500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "1501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:06<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  38.0     0.0     0.0   \n",
      "1           2.0     1.0       1.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "1                   1.0           1.0     1.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0   8.0        5.0     4.0   \n",
      "1       0.0       1.0  1.0  13.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02323466, -0.01180313, -0.077059835, -0.002...  \n",
      "1  [0.020149186, -0.023070963, -0.07817461, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 503it [45:29, 909.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 3: 500 documents inserted.\n",
      "                                          combined_info\n",
      "2000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "2001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:06<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  24.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  39.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       0.0   \n",
      "1                   0.0           0.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  8.0        4.0     8.0   \n",
      "1       0.0       0.0  0.0  7.0        5.0     1.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.025745455, -0.0100526195, -0.07606417, 0.00...  \n",
      "1  [0.022724079, -0.014071822, -0.07715163, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 504it [1:00:39, 909.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 4: 500 documents inserted.\n",
      "                                          combined_info\n",
      "2500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "2501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        0.0  40.0     1.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  23.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0      30.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0   3.0        4.0     5.0   \n",
      "1       0.0       1.0  1.0  10.0        4.0     6.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.024271738, -0.019028971, -0.077365614, -0.0...  \n",
      "1  [0.020461898, -0.01638552, -0.07490797, 0.0022...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 505it [1:15:44, 908.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 5: 500 documents inserted.\n",
      "                                          combined_info\n",
      "3000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "3001  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:00<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "1           2.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       3.0   \n",
      "1                   1.0           1.0     0.0  ...      4.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  11.0        6.0     8.0   \n",
      "1      20.0       0.0  0.0  11.0        6.0     6.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021432409, -0.013125011, -0.07944139, -0.00...  \n",
      "1  [0.022299005, -0.020816423, -0.075494304, 0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 506it [1:30:49, 906.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 6: 500 documents inserted.\n",
      "                                          combined_info\n",
      "3500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "3501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:07<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  25.0     1.0     0.0   \n",
      "1           0.0     1.0       0.0        1.0  22.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       7.0       0.0  1.0  8.0        6.0     8.0   \n",
      "1       2.0       0.0  1.0  8.0        5.0     3.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021391436, -0.015417598, -0.07517676, 0.000...  \n",
      "1  [0.02419361, -0.010329009, -0.076397695, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 507it [1:46:02, 909.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 7: 500 documents inserted.\n",
      "                                          combined_info\n",
      "4000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n",
      "4001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:09<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       1.0        1.0  27.0     0.0     0.0   \n",
      "1           0.0     0.0       1.0        1.0  37.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      2.0       0.0   \n",
      "1                   1.0           0.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0   3.0        4.0     7.0   \n",
      "1       2.0       0.0  1.0  10.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.024849609, -0.012472721, -0.076638, 3.52983...  \n",
      "1  [0.021583777, -0.014947215, -0.078158066, -0.0...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 508it [2:01:16, 910.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 8: 500 documents inserted.\n",
      "                                          combined_info\n",
      "4500  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "4501  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:13<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       0.0        1.0  31.0     1.0     0.0   \n",
      "1           2.0     1.0       0.0        0.0  36.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      5.0      30.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       7.0       0.0  1.0  12.0        6.0     6.0   \n",
      "1      30.0       1.0  0.0   9.0        5.0     2.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021027742, -0.021490235, -0.07777866, -0.00...  \n",
      "1  [0.025439246, -0.018739127, -0.07689765, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 509it [2:16:34, 912.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 9: 500 documents inserted.\n",
      "                                          combined_info\n",
      "5000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "5001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:02<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  30.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  11.0        4.0     6.0   \n",
      "1       0.0       0.0  0.0  10.0        4.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.020579929, -0.014105658, -0.0814815, -0.001...  \n",
      "1  [0.021050557, -0.015296114, -0.076930575, 0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 510it [2:31:40, 910.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 10: 500 documents inserted.\n",
      "                                          combined_info\n",
      "5500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "5501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  18.0     0.0     0.0   \n",
      "1           0.0     0.0       1.0        1.0  32.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      1.0       2.0   \n",
      "1                   0.0           1.0     1.0  ...      3.0       1.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       2.0       0.0  0.0  3.0        6.0     7.0   \n",
      "1       1.0       0.0  0.0  7.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.023904279, -0.009723195, -0.07740682, 0.001...  \n",
      "1  [0.024270702, -0.009409835, -0.07504707, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 511it [2:46:47, 909.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 11: 500 documents inserted.\n",
      "                                          combined_info\n",
      "6000  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "6001  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [14:56<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1           0.0     1.0       0.0        1.0  29.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       2.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       1.0  0.0  10.0        5.0     5.0   \n",
      "1       0.0       0.0  1.0  10.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.021072999, -0.022104833, -0.07585906, -0.00...  \n",
      "1  [0.021860268, -0.014979606, -0.07674988, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 512it [3:01:47, 906.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 12: 500 documents inserted.\n",
      "                                          combined_info\n",
      "6500  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "6501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [1:30:24<00:00, 10.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       0.0        1.0  29.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  22.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0      10.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  7.0        5.0     8.0   \n",
      "1       0.0       0.0  0.0  9.0        5.0     5.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022379091, -0.010802044, -0.07558755, 0.003...  \n",
      "1  [0.025000542, -0.012414336, -0.07771204, 0.002...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 513it [4:32:14, 2276.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 13: 500 documents inserted.\n",
      "                                          combined_info\n",
      "7000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "7001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [15:12<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  26.0     1.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  30.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "1                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       0.0       0.0  1.0  1.0        4.0     8.0   \n",
      "1       1.0       0.0  0.0  5.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.02264808, -0.011972407, -0.075907245, 0.000...  \n",
      "1  [0.023667276, -0.012453735, -0.07615579, 0.000...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 514it [4:47:30, 1865.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 14: 500 documents inserted.\n",
      "                                          combined_info\n",
      "7500  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...\n",
      "7501  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [14:52<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           2.0     1.0       0.0        1.0  36.0     0.0     0.0   \n",
      "1           0.0     1.0       1.0        1.0  36.0     1.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           0.0     1.0  ...      2.0       0.0   \n",
      "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
      "0       4.0       0.0  1.0  9.0        6.0     8.0   \n",
      "1       0.0       1.0  1.0  9.0        6.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 2.0 Highbp: 1.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.022964716, -0.0180326, -0.07549729, -0.0014...  \n",
      "1  [0.022559827, -0.014871327, -0.07770404, -0.00...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 515it [5:02:24, 1572.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 15: 500 documents inserted.\n",
      "                                          combined_info\n",
      "8000  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n",
      "8001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [31:48<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       1.0        1.0  33.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        1.0  23.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     1.0  ...      3.0       0.0   \n",
      "1                   0.0           1.0     1.0  ...      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  10.0        4.0     8.0   \n",
      "1       0.0       0.0  0.0   4.0        5.0     8.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.023811907, -0.0103704175, -0.07728882, 0.00...  \n",
      "1  [0.02427085, -0.00991575, -0.07487146, 0.00229...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 516it [5:34:18, 1675.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 16: 500 documents inserted.\n",
      "                                          combined_info\n",
      "8500  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n",
      "8501  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [54:59<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     0.0       0.0        1.0  32.0     0.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  23.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
      "0                   0.0           1.0     0.0  ...      1.0       0.0   \n",
      "1                   1.0           1.0     0.0  ...      2.0      15.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
      "0       0.0       0.0  0.0  10.0        6.0     4.0   \n",
      "1       0.0       0.0  1.0  10.0        6.0     5.0   \n",
      "\n",
      "                                       combined_info  \\\n",
      "0  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "1  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 0.0 Ch...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.025151163, -0.009721973, -0.07913976, -0.00...  \n",
      "1  [0.02159506, -0.01475258, -0.07843608, 0.00333...  \n",
      "\n",
      "[2 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 517it [6:29:19, 2164.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted batch 1: 500 documents\n",
      "üìå Chunk 17: 500 documents inserted.\n",
      "                                          combined_info\n",
      "9000  Diabetes_012: 0.0 Highbp: 1.0 Highchol: 1.0 Ch...\n",
      "9001  Diabetes_012: 0.0 Highbp: 0.0 Highchol: 1.0 Ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# === Local Files Folde7r ===\n",
    "LOCAL_FOLDER = \"../data/arc\" \n",
    "for file_name in os.listdir(LOCAL_FOLDER):\n",
    "    file_path = os.path.join(LOCAL_FOLDER, file_name)\n",
    "\n",
    "    # Skip non-data files\n",
    "    if not file_name.lower().endswith(('.csv', '.xlsx', '.xls', '.json')):\n",
    "        print(f\"Skipping unsupported file: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    datasets_col = db[\"datasets\"]\n",
    "    data_col = db[\"records_embeddings\"]\n",
    "\n",
    "    #Check if file already uploaded\n",
    "    existing = datasets_col.find_one({\"file_name\": file_name})\n",
    "    if existing:\n",
    "         print(f\"‚è≠Ô∏è Skipping {file_name} (already uploaded)\")\n",
    "         #continue\n",
    "\n",
    "    print(f\"üìÇ Processing {file_name}...\")\n",
    "    # Initialize metadata tracking\n",
    "    first_chunk = None\n",
    "    total_rows = 0\n",
    "    combined_missing = None\n",
    "\n",
    "    # Read file into DataFrame\n",
    "    try:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            chunk_iter = pd.read_csv(file_path, chunksize=500)\n",
    "        elif file_name.endswith((\".xlsx\", \".xls\")):\n",
    "            chunk_iter = pd.read_excel(file_path, chunksize=500)\n",
    "        elif file_name.endswith(\".json\"):\n",
    "            chunk_iter = pd.read_json(file_path, lines=True, chunksize=500)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "    dataset_id = None\n",
    "\n",
    "    # === Insert Metadata ===\n",
    "    start_index = 500\n",
    "\n",
    "    # Skip the first `start_index // chunksize` chunks\n",
    "    chunks_to_embed = itertools.islice(chunk_iter, start_index // 500, None)\n",
    "    \n",
    "    # Now embed starting from chunk N\n",
    "    #for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks_to_embed, desc=\"Embedding\", initial=start_index, total=None)):        \n",
    "        if first_chunk is None:\n",
    "            first_chunk = chunk.copy()\n",
    "            combined_missing = chunk.isnull().sum()\n",
    "            dataset_doc = {\n",
    "                \"file_name\": file_name,\n",
    "                \"upload_date\": datetime.now(),\n",
    "                \"n_columns\": chunk.shape[1],\n",
    "                \"columns\": chunk.columns.tolist(),\n",
    "                \"missing_values\": chunk.isnull().sum().to_dict(),\n",
    "                \"file_type\": os.path.splitext(file_name)[-1].replace(\".\", \"\"),\n",
    "                \"file_path\": file_path,\n",
    "                \"column_types\": chunk.dtypes.astype(str).to_dict(),\n",
    "            }\n",
    "            dataset_id = datasets_col.insert_one(dataset_doc).inserted_id\n",
    "            print(f\"‚úÖ Inserted metadata for {file_name}\")\n",
    "\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "        # === Combine all attributes ===\n",
    "        chunk = combine_all_attributes(chunk, exclude_columns=[])\n",
    "        print(chunk[[\"combined_info\"]].head(2))  # preview\n",
    "    \n",
    "        duplicated_data = []\n",
    "        for row in tqdm(chunk.itertuples(index=False), total=len(chunk), desc=\"Embedding\"):\n",
    "            duplicated_rows = get_embedding(row._asdict())\n",
    "            duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "        if duplicated_data:\n",
    "            df_dup = pd.DataFrame(duplicated_data)\n",
    "            print(df_dup.head(2))\n",
    "            try:\n",
    "                total_inserted_tabular = insert_df_to_mongodb(df_dup, data_col, dataset_id)\n",
    "                print(\n",
    "                    f\"üìå Chunk {chunk_idx + 1}: {len(df_dup)} documents inserted.\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error inserting chunk {chunk_idx + 1}: {e}\")\n",
    "\n",
    "    if dataset_id:\n",
    "        datasets_col.update_one(\n",
    "            {\"_id\": dataset_id},\n",
    "            {\"$set\": {\"n_rows\": total_rows}}\n",
    "        )\n",
    "        print(f\"‚úÖ Finalized metadata for {file_name}: {total_rows} rows\")\n",
    "\n",
    "    # # Combine all columns into 'combined_info'\n",
    "    # df = combine_all_attributes(df, exclude_columns=[])  \n",
    "    # print(df[[\"combined_info\"]].head(2))  # preview\n",
    "\n",
    "    # # Apply the function and expand the dataset\n",
    "    # duplicated_data = []\n",
    "    # for _, row in tqdm(\n",
    "    #     df.iterrows(),\n",
    "    #     desc=\"Generating embeddings and duplicating rows\",\n",
    "    #     total=len(df),\n",
    "    # ):\n",
    "    #     duplicated_rows = get_embedding(row)\n",
    "    #     duplicated_data.extend(duplicated_rows)\n",
    "\n",
    "    # # Create a new DataFrame from the duplicated data\n",
    "    # df = pd.DataFrame(duplicated_data)\n",
    "    #print(df.head(2))\n",
    "\n",
    "    # === Insert Data Records ===\n",
    "    # Insert dataframe to mongodb\n",
    "    # try:\n",
    "    #     total_inserted_tabular = insert_df_to_mongodb(df, data_col, dataset_id)\n",
    "    #     print(\n",
    "    #         f\"{file_name} data ingestion completed. Total documents inserted: {total_inserted_tabular}\"\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred while inserting {file_name}: {e}\")\n",
    "    #     print(\"Pandas version:\", pd.__version__)\n",
    "    # print_dataframe_info(df, {file_name})\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n‚úÖ Ingestion process completed for all {total_inserted_tabular} files.\")\n",
    "# print(\"\\nInsertion Summary:\")\n",
    "# print(\n",
    "#     f\"Tabular files inserted: {total_inserted_tabular if 'total_inserted_tabular' in locals() else 'Failed'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "    # data_records = df.to_dict(orient=\"records\")\n",
    "    # for record in data_records:\n",
    "    #     record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # if data_records:\n",
    "    #     data_col.insert_many(data_records)\n",
    "    #     print(f\"‚úÖ Uploaded data records for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "\n",
    "def insert_df_to_mongodb(df, collection,dataset_id,  batch_size=1000):\n",
    "    \"\"\"\n",
    "    Insert a pandas DataFrame into a MongoDB collection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to insert\n",
    "    collection (pymongo.collection.Collection): The MongoDB collection to insert into\n",
    "    batch_size (int): Number of documents to insert in each batch\n",
    "\n",
    "    Returns:\n",
    "    int: Number of documents successfully inserted\n",
    "    \"\"\"\n",
    "    total_inserted = 0\n",
    "\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict(\"records\")\n",
    "\n",
    "    for record in records:\n",
    "        record[\"dataset_id\"] = dataset_id\n",
    "\n",
    "    # Insert in batches\n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i : i + batch_size]\n",
    "        try:\n",
    "            result = collection.insert_many(batch, ordered=False)\n",
    "            total_inserted += len(result.inserted_ids)\n",
    "            print(\n",
    "                f\"Inserted batch {i//batch_size + 1}: {len(result.inserted_ids)} documents\"\n",
    "            )\n",
    "        except BulkWriteError as bwe:\n",
    "            total_inserted += bwe.details[\"nInserted\"]\n",
    "            print(\n",
    "                f\"Batch {i//batch_size + 1} partially inserted. {bwe.details['nInserted']} inserted, {len(bwe.details['writeErrors'])} failed.\"\n",
    "            )\n",
    "\n",
    "    return total_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataframe_info(df, df_name):\n",
    "    print(f\"\\n{df_name} DataFrame info:\")\n",
    "    print(df.info())\n",
    "    print(f\"\\nFirst few rows of the {df_name} DataFrame:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512      # Good default for embeddings\n",
    "OVERLAP = 50          # Helps with context continuity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=MAX_TOKENS, overlap=OVERLAP):\n",
    "    \"\"\"\n",
    "    Split the text into overlapping chunks based on token count.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk_tokens = tokens[i : i + max_tokens]\n",
    "        chunk = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input_data, model=GEMINI_EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the 'combined_attributes' column and duplicate the row for each chunk\n",
    "    or generate embeddings for a given string.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, str):\n",
    "        text = input_data\n",
    "    else:\n",
    "        text = input_data[\"combined_info\"]\n",
    "\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    # Split text into chunks if it's too long\n",
    "    chunks = chunk_text(text)\n",
    "\n",
    "    # Embed each chunk\n",
    "    chunk_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        time.sleep(1.5)  # 1.5 seconds delay per request\n",
    "        chunk = chunk.replace(\"\\n\", \" \")\n",
    "        response = genai.embed_content(\n",
    "                model=model,\n",
    "                content=chunk,\n",
    "                task_type=\"retrieval_document\",  # optional, depends on your use case\n",
    "                request_options={\"timeout\": 60}  # increase timeout in seconds\n",
    "            )\n",
    "        embedding = response[\"embedding\"]\n",
    "        #embedding = embedding_model.embed_query(text=chunk)\n",
    "        chunk_embeddings.append(embedding)\n",
    "\n",
    "    if isinstance(input_data, str):\n",
    "        # Return list of embeddings for string input\n",
    "        return chunk_embeddings[0]\n",
    "    # Create duplicated rows for each chunk with the respective embedding for row input\n",
    "    duplicated_rows = []\n",
    "    for embedding in chunk_embeddings:\n",
    "        new_row = input_data.copy()\n",
    "        new_row[\"embedding\"] = embedding\n",
    "        duplicated_rows.append(new_row)\n",
    "    return duplicated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding(\"example text\")  \n",
    "print(len(embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vector search index definition\n",
    "vector_search_index_definition = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": False,\n",
    "        \"fields\": {\n",
    "            \"embedding\": {\n",
    "                \"dimensions\": 768,\n",
    "                \"similarity\": \"cosine\",\n",
    "                \"type\": \"knnVector\",\n",
    "            },\n",
    "            \"recordId\": {\"type\": \"string\"},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically create vector search index for both colelctions\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "\n",
    "def setup_vector_search_index_with_filter(\n",
    "    collection, index_definition, index_name=\"csv_vector_index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup a vector search index for a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    index_definition: Dictionary containing the index definition\n",
    "    index_name: Name of the index (default: \"vector_index_with_filter\")\n",
    "    \"\"\"\n",
    "    new_vector_search_index_model = SearchIndexModel(\n",
    "        definition=index_definition,\n",
    "        name=index_name,\n",
    "    )\n",
    "\n",
    "    # Create the new index\n",
    "    try:\n",
    "        result = collection.create_search_index(model=new_vector_search_index_model)\n",
    "        print(f\"Creating index '{index_name}'...\")\n",
    "        # time.sleep(20)  # Sleep for 20 seconds\n",
    "        print(f\"New index '{index_name}' created successfully:\", result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating new vector search index '{index_name}': {e!s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index 'csv_vector_index'...\n",
      "New index 'csv_vector_index' created successfully: csv_vector_index\n"
     ]
    }
   ],
   "source": [
    "setup_vector_search_index_with_filter(\n",
    "    collection, vector_search_index_definition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    vector_search_stage = {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"csv_vector_index\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"embedding\",\n",
    "            \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "            \"limit\": 5,  # Return top 4 matches\n",
    "        }\n",
    "    }\n",
    "\n",
    "    unset_stage = {\n",
    "        \"$unset\": \"embedding\"  # Exclude the 'embedding' field from the results\n",
    "    }\n",
    "\n",
    "    project_stage = {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,  # Exclude the _id field,\n",
    "            \"combined_info\": 1,\n",
    "            \"score\": {\n",
    "                \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pipeline = [vector_search_stage, unset_stage, project_stage]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_search_result(query, collection):\n",
    "    get_knowledge = vector_search(query, collection)\n",
    "    search_results = []\n",
    "    for result in get_knowledge:\n",
    "        search_results.append(\n",
    "            [result.get(\"score\", \"N/A\"), result.get(\"combined_info\", \"N/A\")]\n",
    "        )\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Get me a record having high blood sugar\n",
      "\n",
      "Continue to answer the query by using the Search Results:\n",
      "\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|   Similarity Score | Combined Information                                                                                                                                 |\n",
      "+====================+======================================================================================================================================================+\n",
      "|           0.902344 | Pregnancies: 4.0 Glucose: 114.0 Bloodpressure: 64.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 28.9 Diabetespedigreefunction: 0.126 Age: 24.0 Outcome: 0.0 |\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|           0.90215  | Pregnancies: 5.0 Glucose: 143.0 Bloodpressure: 78.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 45.0 Diabetespedigreefunction: 0.19 Age: 47.0 Outcome: 0.0  |\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|           0.901845 | Pregnancies: 6.0 Glucose: 166.0 Bloodpressure: 74.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 26.6 Diabetespedigreefunction: 0.304 Age: 66.0 Outcome: 0.0 |\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|           0.901512 | Pregnancies: 5.0 Glucose: 122.0 Bloodpressure: 86.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 34.7 Diabetespedigreefunction: 0.29 Age: 33.0 Outcome: 0.0  |\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|           0.901343 | Pregnancies: 5.0 Glucose: 136.0 Bloodpressure: 82.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 0.0 Diabetespedigreefunction: 0.64 Age: 69.0 Outcome: 0.0   |\n",
      "+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = \"Get me a record having high blood sugar\"\n",
    "source_information = get_vector_search_result(query, collection)\n",
    "\n",
    "table_headers = [\"Similarity Score\", \"Combined Information\"]\n",
    "table = tabulate.tabulate(source_information, headers=table_headers, tablefmt=\"grid\")\n",
    "\n",
    "combined_information = f\"\"\"Query: {query}\n",
    "\n",
    "Continue to answer the query by using the Search Results:\n",
    "\n",
    "{table}\n",
    "\"\"\"\n",
    "\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install --quiet -U langchain langchain_mongodb langgraph langsmith motor langchain_anthropic # langchain-groq\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.operations import SearchIndexModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programatically create search indexes\n",
    "\n",
    "\n",
    "def create_collection_search_index(collection, index_definition, index_name):\n",
    "    \"\"\"\n",
    "    Create a search index for a MongoDB Atlas collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    index_definition: Dictionary defining the index mappings\n",
    "    index_name: String name for the index\n",
    "\n",
    "    Returns:\n",
    "    str: Result of the index creation operation\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        search_index_model = SearchIndexModel(\n",
    "            definition=index_definition, name=index_name\n",
    "        )\n",
    "\n",
    "        result = collection.create_search_index(model=search_index_model)\n",
    "        print(f\"Search index '{index_name}' created successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating search index: {e!s}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_collection_search_indexes(collection):\n",
    "    \"\"\"\n",
    "    Print all search indexes for a given collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    \"\"\"\n",
    "    print(f\"\\nSearch indexes for collection '{collection.name}':\")\n",
    "    for index in collection.list_search_indexes():\n",
    "        print(f\"Index: {index['name']}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index 'records_text_search_index' created successfully\n",
      "\n",
      "Search indexes for collection 'records_embeddings':\n",
      "Index: csv_vector_index\n",
      "Index: records_text_search_index\n"
     ]
    }
   ],
   "source": [
    "collection_text_index_definition = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": False,\n",
    "        \"fields\": {\n",
    "            \"combined_info\": {\"type\": \"string\"},\n",
    "            \"dataset_id\": {\"type\": \"objectId\"},\n",
    "            \n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "create_collection_search_index(\n",
    "    collection,\n",
    "    collection_text_index_definition,\n",
    "    \"records_text_search_index\",\n",
    ")\n",
    "\n",
    "# Print all indexes in the collection\n",
    "print_collection_search_indexes(collection)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.mongodb_client import MONGODB_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = os.getenv(\"MONGO_DB\")\n",
    "ATLAS_VECTOR_SEARCH_INDEX = \"csv_vector_index\"\n",
    "CSV_COLLECTION= \"records_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=GEMINI_EMBEDDING_MODEL,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Get me a record having high blood sugar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Stores Intialisation\n",
    "vector_store_csv_files = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGODB_URI,\n",
    "    namespace=DB_NAME + \".\" + CSV_COLLECTION,\n",
    "    embedding=embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX,\n",
    "    text_key=\"combined_info\",\n",
    ")\n",
    "\n",
    "hybrid_search = MongoDBAtlasHybridSearchRetriever(\n",
    "    vectorstore=vector_store_csv_files,\n",
    "    search_index_name=\"records_text_search_index\",\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_search_result = hybrid_search.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_results_to_table(search_results):\n",
    "    \"\"\"\n",
    "    Convert hybrid search results to a formatted markdown table.\n",
    "\n",
    "    Args:\n",
    "    search_results (list): List of Document objects containing search results\n",
    "\n",
    "    Returns:\n",
    "    str: Formatted markdown table of search results\n",
    "    \"\"\"\n",
    "    # Extract relevant information from each result\n",
    "    data = []\n",
    "    for rank, doc in enumerate(search_results, start=1):\n",
    "        metadata = doc.metadata\n",
    "        #print(metadata)\n",
    "\n",
    "        # Start with rank and then unpack all metadata\n",
    "        row = {\"Rank\": rank}\n",
    "        # Optionally round numerical fields if they exist\n",
    "        for key in [\"vector_score\", \"fulltext_score\", \"score\"]:\n",
    "            if key in row:\n",
    "                row[key] = round(metadata[key], 5)\n",
    "        # Add other metadata fields dynamically\n",
    "        for k, v in metadata.items():\n",
    "            if k not in row:  # avoid overwriting scores and Rank\n",
    "                row[k] = v\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "        # data.append(\n",
    "        #     {\n",
    "        #         \"Rank\": rank,\n",
    "        #         #\"Procedure ID\": metadata[\"procedureId\"],\n",
    "        #         \"File_name\": metadata[\"file_name\"],\n",
    "        #         #\"Category\": metadata[\"category\"],\n",
    "        #         \"Vector Score\": round(metadata[\"vector_score\"], 5),\n",
    "        #         \"Full-text Score\": round(metadata[\"fulltext_score\"], 5),\n",
    "        #         \"Total Score\": round(metadata[\"score\"], 5),\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Reorder columns if needed\n",
    "    columns_order = [\"Rank\"] + [col for col in df.columns if col != \"Rank\"]\n",
    "    df = df[columns_order]\n",
    "\n",
    "    # Print all columns in full\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "    # Generate markdown table\n",
    "    table = tabulate.tabulate(df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Rank | _id                      |   Pregnancies |   Glucose |   BloodPressure |   SkinThickness |   Insulin |   BMI |   DiabetesPedigreeFunction |   Age |   Outcome | dataset_id               |   vector_score |   rank |   fulltext_score |     score |\n",
      "|-------:|:-------------------------|--------------:|----------:|----------------:|----------------:|----------:|------:|---------------------------:|------:|----------:|:-------------------------|---------------:|-------:|-----------------:|----------:|\n",
      "|      1 | 685056e712c7b37a8d059d6e |             4 |       114 |              64 |               0 |         0 |  28.9 |                      0.126 |    24 |         0 | 685052b612c7b37a8d059b93 |      0.0163934 |      0 |                0 | 0.0163934 |\n",
      "|      2 | 685056e712c7b37a8d059c46 |             5 |       143 |              78 |               0 |         0 |  45   |                      0.19  |    47 |         0 | 685052b612c7b37a8d059b93 |      0.016129  |      1 |                0 | 0.016129  |\n",
      "|      3 | 685056e712c7b37a8d059d83 |             6 |       166 |              74 |               0 |         0 |  26.6 |                      0.304 |    66 |         0 | 685052b612c7b37a8d059b93 |      0.015873  |      2 |                0 | 0.015873  |\n",
      "|      4 | 685056e712c7b37a8d059ceb |             5 |       122 |              86 |               0 |         0 |  34.7 |                      0.29  |    33 |         0 | 685052b612c7b37a8d059b93 |      0.015625  |      3 |                0 | 0.015625  |\n",
      "|      5 | 685058e512c7b37a8d059e40 |             5 |       136 |              82 |               0 |         0 |   0   |                      0.64  |    69 |         0 | 685052b612c7b37a8d059b93 |      0.0153846 |      4 |                0 | 0.0153846 |\n"
     ]
    }
   ],
   "source": [
    "table = hybrid_search_results_to_table(hybrid_search_result)\n",
    "print(table)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.retrievers import MongoDBAtlasFullTextSearchRetriever\n",
    "\n",
    "full_text_search = MongoDBAtlasFullTextSearchRetriever(\n",
    "    collection=collection,\n",
    "    search_index_name=\"record_text_search_index\",\n",
    "    search_field=\"combined_info\",\n",
    "    top_k=5,\n",
    ")\n",
    "full_text_search_result = full_text_search.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(full_text_search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections.abc import AsyncIterator\n",
    "from contextlib import AbstractContextManager\n",
    "from datetime import datetime, timezone\n",
    "from types import TracebackType\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.base import (\n",
    "    BaseCheckpointSaver,\n",
    "    Checkpoint,\n",
    "    CheckpointMetadata,\n",
    "    CheckpointTuple,\n",
    "    SerializerProtocol,\n",
    ")\n",
    "from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "from typing_extensions import Self\n",
    "\n",
    "class JsonPlusSerializerCompat(JsonPlusSerializer):\n",
    "    def loads(self, data: bytes) -> Any:\n",
    "        if data.startswith(b\"\\x80\") and data.endswith(b\".\"):\n",
    "            return pickle.loads(data)\n",
    "        return super().loads(data)\n",
    "    \n",
    "class MongoDBSaver(AbstractContextManager, BaseCheckpointSaver):\n",
    "    serde = JsonPlusSerializerCompat()\n",
    "\n",
    "    client: AsyncIOMotorClient\n",
    "    db_name: str\n",
    "    collection_name: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AsyncIOMotorClient,\n",
    "        db_name: str,\n",
    "        collection_name: str,\n",
    "        *,\n",
    "        serde: Optional[SerializerProtocol] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(serde=serde)\n",
    "        self.client = client\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.collection = client[db_name][collection_name]\n",
    "\n",
    "    def __enter__(self) -> Self:\n",
    "        return self\n",
    "\n",
    "    def __exit__(\n",
    "        self,\n",
    "        __exc_type: Optional[type[BaseException]],\n",
    "        __exc_value: Optional[BaseException],\n",
    "        __traceback: Optional[TracebackType],\n",
    "    ) -> Optional[bool]:\n",
    "        return True\n",
    "    \n",
    "    async def aget_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            query = {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": config[\"configurable\"][\"thread_ts\"],\n",
    "            }\n",
    "        else:\n",
    "            query = {\"thread_id\": config[\"configurable\"][\"thread_id\"]}\n",
    "\n",
    "        doc = await self.collection.find_one(query, sort=[(\"thread_ts\", -1)])\n",
    "        if doc:\n",
    "            return CheckpointTuple(\n",
    "                config,\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    async def alist(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> AsyncIterator[CheckpointTuple]:\n",
    "        query = {}\n",
    "        if config is not None:\n",
    "            query[\"thread_id\"] = config[\"configurable\"][\"thread_id\"]\n",
    "        if filter:\n",
    "            for key, value in filter.items():\n",
    "                query[f\"metadata.{key}\"] = value\n",
    "        if before is not None:\n",
    "            query[\"thread_ts\"] = {\"$lt\": before[\"configurable\"][\"thread_ts\"]}\n",
    "\n",
    "        cursor = self.collection.find(query).sort(\"thread_ts\", -1)\n",
    "        if limit:\n",
    "            cursor = cursor.limit(limit)\n",
    "\n",
    "        async for doc in cursor:\n",
    "            yield CheckpointTuple(\n",
    "                {\n",
    "                    \"configurable\": {\n",
    "                        \"thread_id\": doc[\"thread_id\"],\n",
    "                        \"thread_ts\": doc[\"thread_ts\"],\n",
    "                    }\n",
    "                },\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    async def aput(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "        new_versions: Optional[dict[str, Union[str, float, int]]],\n",
    "    ) -> RunnableConfig:\n",
    "        doc = {\n",
    "            \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "            \"thread_ts\": checkpoint[\"id\"],\n",
    "            \"checkpoint\": self.serde.dumps(checkpoint),\n",
    "            \"metadata\": self.serde.dumps(metadata),\n",
    "        }\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            doc[\"parent_ts\"] = config[\"configurable\"][\"thread_ts\"]\n",
    "        await self.collection.insert_one(doc)\n",
    "        return {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": checkpoint[\"id\"],\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Implement synchronous methods as well for compatibility\n",
    "    def get_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        raise NotImplementedError(\"Use aget_tuple for asynchronous operations\")\n",
    "\n",
    "    def list(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Use alist for asynchronous operations\")\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "    ) -> RunnableConfig:\n",
    "        raise NotImplementedError(\"Use aput for asynchronous operations\")\n",
    "\n",
    "    async def aput_writes(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        writes: List[Tuple[str, Any]],\n",
    "        task_id: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Asynchronously store intermediate writes linked to a checkpoint.\"\"\"\n",
    "        docs = []\n",
    "        for channel, value in writes:\n",
    "            doc = {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"task_id\": task_id,\n",
    "                \"channel\": channel,\n",
    "                \"value\": self.serde.dumps(value),\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            }\n",
    "            docs.append(doc)\n",
    "\n",
    "        if docs:\n",
    "            await self.collection.insert_many(docs)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def csv_files_vector_search_tool(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search on safety procedures.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        k (int, optional): Number of top results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples (Document, score), where Document is a record\n",
    "              and score is the similarity score (lower is more similar).\n",
    "\n",
    "    Note:\n",
    "        Uses the global vector_store_csv_files for the search.\n",
    "    \"\"\"\n",
    "\n",
    "    vector_search_results = vector_store_csv_files.similarity_search_with_score(\n",
    "        query=query, k=k\n",
    "    )\n",
    "    return vector_search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def hybrid_search_tool(query: str):\n",
    "    \"\"\"\n",
    "    Perform a hybrid (vector + full-text) search on safety procedures.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        list: Relevant safety procedure documents from hybrid search.\n",
    "\n",
    "    Note:\n",
    "        Uses both vector_store_csv_files and record_text_search_index.\n",
    "    \"\"\"\n",
    "\n",
    "    hybrid_search = MongoDBAtlasHybridSearchRetriever(\n",
    "        vectorstore=vector_store_csv_files,\n",
    "        search_index_name=\"record_text_search_index\",\n",
    "        top_k=5,\n",
    "    )\n",
    "\n",
    "    hybrid_search_result = hybrid_search.get_relevant_documents(query)\n",
    "\n",
    "    return hybrid_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "## Generalized Record Document Creator\n",
    "class GenericRecord(BaseModel):\n",
    "    # Flexible for any CSV columns\n",
    "    data: Dict[str, Any]\n",
    "    combined_info: Optional[str] = None\n",
    "    embedding: Optional[List[float]] = None\n",
    "    dataset_id: Optional[str] = None\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "## Function to Create the Document\n",
    "def create_generic_record_document(row: Dict[str, Any], dataset_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Create a standardized document for any CSV record with flexible columns.\n",
    "\n",
    "    Args:\n",
    "        row (Dict[str, Any]): The actual CSV row as a dictionary.\n",
    "        dataset_id (str): Reference to the parent dataset document.\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned and standardized MongoDB document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean keys: remove leading/trailing whitespace, lowercase, etc. (optional)\n",
    "        cleaned_row = {k.strip(): v for k, v in row.items()}\n",
    "\n",
    "        # Create combined_info string for text search\n",
    "        combined_info = \" \".join(f\"{k}: {v}\" for k, v in cleaned_row.items())\n",
    "\n",
    "        # Package into a generic Pydantic record\n",
    "        record = GenericRecord(\n",
    "            data=cleaned_row,\n",
    "            combined_info=combined_info,\n",
    "            dataset_id=dataset_id\n",
    "        )\n",
    "\n",
    "        return record.dict()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid record data: {e!s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Tool to add new record\n",
    "@tool\n",
    "def create_new_record(new_record: Dict[str, any]) -> dict:\n",
    "    \"\"\"\n",
    "    Create and validate a new generic CSV record.\n",
    "\n",
    "    Args:\n",
    "        new_record (dict): Dictionary containing a row from a CSV file. \n",
    "                           Must include 'dataset_id' as a key.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validated and formatted record document.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input data is invalid or incomplete.\n",
    "\n",
    "    Note:\n",
    "        Uses Pydantic for data validation via create_generic_record_document function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset_id = new_record.pop(\"dataset_id\", None)\n",
    "        if not dataset_id:\n",
    "            raise ValueError(\"Missing required field 'dataset_id' in the new record.\")\n",
    "\n",
    "        document = create_generic_record_document(new_record, dataset_id)\n",
    "        return document\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error creating new record: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_embeddings_collection_tools = [\n",
    "    csv_files_vector_search_tool,\n",
    "    #hybrid_search_tool,\n",
    "    create_new_record,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Article Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever.rag_retriever import vector_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"diabetes_data\"\n",
    "COLLECTION_NAME = \"docs_multimodal\"\n",
    "VS_INDEX_NAME = \"multimodal_vector_index\"\n",
    "# Connect to the MongoDB collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def article_page_vector_search_tool(\n",
    "    query: str,\n",
    "    model: str = \"sbert\",\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    )-> str:\n",
    "    \"\"\"\n",
    "    Search academic papers (page-level) using a text query via multimodal embeddings.\n",
    "    Returns the top 5 page-level summaries with citations.\n",
    "\n",
    "    Args:\n",
    "        query (str): The textual search query.\n",
    "        model (str): Embedding model (\"sbert\", \"clip\", \"voyage\").\n",
    "        collection (str): MongoDB collection name to search in.\n",
    "\n",
    "    Returns:\n",
    "        str: Top 5 matching pages, each with citation and summary.\n",
    "        \n",
    "    \"\"\"\n",
    "    collection_ref = mongodb_client[DB_NAME][collection_name]\n",
    "    results = vector_search(query, model=model, collection=collection_ref, display_images=False)\n",
    "    if not results:\n",
    "        return \"No relevant pages found.\"\n",
    "    \n",
    "    summaries = []\n",
    "    for r in results[:5]:\n",
    "        title = r.get(\"pdf_title\", \"Unknown Title\")\n",
    "        page = r.get(\"page_number\", \"?\")\n",
    "        text = r.get(\"summary\") or r.get(\"page_text\", \"\")[:300]\n",
    "        summaries.append(f\"[{title}, Page {page}]: {text.strip()}\")\n",
    "\n",
    "    return \"\\n\\n\".join(summaries)  # return top 5 summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\") #gemini-pro-vision\n",
    "\n",
    "def extract_info_from_page_image(image: Image.Image, pdf_title: str, page_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Use Gemini to extract citations, figures/tables, and a summary from a page image.\n",
    "    \"\"\"\n",
    "    #response = requests.get(gcs_url)\n",
    "    #image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing a scientific paper page image from the PDF titled: \"{pdf_title}\", page {page_number}.\n",
    "    Please extract the following:\n",
    "    1. All citations on the page (e.g., [1], Smith et al. 2020).\n",
    "    2. Any tables or figures, including titles or captions.\n",
    "    3. A brief summary of the page content.\n",
    "    \n",
    "    Return output in markdown with the following format:\n",
    "    **Citations:** ...\n",
    "    **Figures/Tables:** ...\n",
    "    **Summary:** ...\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content([prompt, image])\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Gemini processing error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.gcs_utils import get_image_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PROJECT = os.getenv(\"GCS_PROJECT\")\n",
    "GCS_BUCKET = os.getenv(\"GCS_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GCS client and bucket\n",
    "gcs_client = storage.Client(project=GCS_PROJECT)\n",
    "gcs_bucket = gcs_client.bucket(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "\n",
    "def get_image_from_gcs(gcs_bucket, key: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Download image bytes from GCS.\n",
    "\n",
    "    Args:\n",
    "        gcs_bucket: GCS bucket instance.\n",
    "        key (str): Blob key in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        bytes: Image bytes.\n",
    "    \"\"\"\n",
    "    blob = gcs_bucket.blob(key)\n",
    "    return blob.download_as_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from langchain.agents import tool\n",
    "#from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "@tool\n",
    "def vector_search_image_tool(\n",
    "    collection_name: str,\n",
    "    image_bytes: Optional[bytes] = None,\n",
    "    text_query: Optional[str] = None,  \n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Search academic papers using either an image or text query. If text is provided, it generates\n",
    "    a CLIP embedding from the query and searches against page images. If image is provided, it\n",
    "    searches using image embeddings.\n",
    "\n",
    "    Args:\n",
    "        image_bytes (bytes, optional): Query image content.\n",
    "        text_query (str, optional): Text query to generate image embedding.\n",
    "        collection_name (str): MongoDB collection name.\n",
    "\n",
    "    Returns:\n",
    "        str: Top 5 matching pages with citation and Gemini summary.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    collection_ref = db[collection_name]\n",
    "\n",
    "    # Determine search mode\n",
    "    if image_bytes:\n",
    "        try:\n",
    "            query_image = Image.open(BytesIO(image_bytes))\n",
    "        except Exception as e:\n",
    "            return f\"Invalid image input: {e}\"\n",
    "        results = vector_search(query_image, model=\"clip_image\", collection=collection_ref, display_images=False)\n",
    "\n",
    "    elif text_query:\n",
    "        results = vector_search(text_query, model=\"clip\", collection=collection_ref, display_images=False)\n",
    "\n",
    "    else:\n",
    "        return \"Either image_bytes or text_query must be provided.\"\n",
    "    \n",
    "    if not results:\n",
    "        return \"No matching results found.\"\n",
    "        \n",
    "    output = []\n",
    "    for r in results[:5]:\n",
    "        pdf_title = r.get(\"pdf_title\", \"Unknown Title\")\n",
    "        page_number = r.get(\"page_number\", -1)\n",
    "        gcs_key = r.get(\"gcs_key\", \"\")\n",
    "        doc_id = r.get(\"_id\")  # Ensure this is present in your search result\n",
    "\n",
    "        # Check for existing gemini_summary in DB\n",
    "        cached_doc = collection_ref.find_one({\"_id\": doc_id}, {\"gemini_summary\": 1})\n",
    "        if cached_doc and cached_doc.get(\"gemini_summary\"):\n",
    "            summary = cached_doc[\"gemini_summary\"]\n",
    "        else:\n",
    "            # If not cached, fetch image + generate Gemini summary\n",
    "            try:\n",
    "                page_bytes = get_image_from_gcs(gcs_bucket, gcs_key)\n",
    "                page_image = Image.open(BytesIO(page_bytes))\n",
    "            except Exception as e:\n",
    "                return f\"Failed to fetch page image: {e}\"\n",
    "            else:\n",
    "                summary = extract_info_from_page_image(page_image, pdf_title, page_number)\n",
    "\n",
    "                # Save summary to DB\n",
    "                collection_ref.update_one(\n",
    "                    {\"_id\": doc_id},\n",
    "                    {\"$set\": {\"gemini_summary\": summary}},\n",
    "                )\n",
    "\n",
    "        citation = f\"### {pdf_title}, Page {page_number}\"\n",
    "        output.append(f\"{citation}\\n{summary.strip()}\")\n",
    "        \n",
    "    return \"\\n\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_multimodal_collection_tools = [article_page_vector_search_tool, vector_search_image_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                            temperature=0,\n",
    "                            max_tokens=None,\n",
    "                            timeout=None,\n",
    "                            max_retries=3,\n",
    "                            # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from datetime import datetime\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "# def create_agent(llm, tools, system_message: str):\n",
    "#     \"\"\"Create an agent.\"\"\"\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "#                 \" Use the provided tools to progress towards answering the question.\"\n",
    "#                 \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "#                 # \" will help where you left off. Execute what you can to make progress.\"\n",
    "#                 \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "#                 \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "#                 \" You have access to the following tools: {tool_names}.\\n{system_message}\"\n",
    "#                 \"\\nCurrent time: {time}.\",\n",
    "#             ),\n",
    "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     prompt = prompt.partial(system_message=system_message)\n",
    "#     prompt = prompt.partial(time=lambda: str(datetime.now()))\n",
    "#     prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "\n",
    "#     return prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Prompt Creation\n",
    "\n",
    "https://github.com/mongodb-developer/GenAI-Showcase/blob/50535ba52c872ed03a975bf180f01f84696e7cc9/notebooks/agents/agent_fireworks_ai_langchain_mongodb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_purpose = \"\"\"\n",
    "You are an intelligent Multimodal Dataset and Document Assistant Agent. You help users interact with structured CSV datasets and unstructured academic articles, especially those in PDF format. These data sources are pre-indexed with vector embeddings and metadata, allowing efficient search and retrieval across both tabular and visual/textual domains.\n",
    "\n",
    "Your key responsibilities include:\n",
    "\n",
    "1. Searching and retrieving from CSV datasets:\n",
    "  - Use search and similarity tools to find relevant rows based on natural language queries.\n",
    "  - Retrieve records with the highest hybrid or semantic similarity scores.\n",
    "  - Present relevant CSV content clearly, along with metadata such as column names and file name.\n",
    "\n",
    "2. Creating new CSV records:\n",
    "  - When provided with a dictionary representing a row, use the `create_new_record` tool to validate and structure it.\n",
    "  - Ensure the new record includes a `dataset_id` and other available fields.\n",
    "  - Construct a `combined_info` string to support downstream embedding and search.\n",
    "\n",
    "3. Handling academic PDF articles:\n",
    "  - Use `article_page_vector_search_tool` to retrieve relevant page-level summaries from academic PDFs using a query.\n",
    "  - Use `vector_search_image_tool` to perform image-based search on PDF pages using CLIP embeddings.\n",
    "  - Present results with clear summaries, proper citations (including PDF title and page number), and structured insight from both page text and image content.\n",
    "\n",
    "4. Interpreting and comparing records:\n",
    "  - Explain the meaning of individual records or fields, in both CSV and PDF contexts.\n",
    "  - Detect patterns across results, such as common entities, table structures, or outliers.\n",
    "  - Help users compare information from CSV datasets and PDF documents when relevant.\n",
    "\n",
    "5. Dataset/document context awareness:\n",
    "  - Understand that CSVs have varying schemas; don't assume fixed columns.\n",
    "  - Use metadata like `file_name`, `n_columns`, `n_rows`, or `pdf_title`, `page_number`, `gcs_key`, and `url` to ground your responses.\n",
    "  - Avoid making assumptions about missing content‚Äîonly respond using indexed or retrieved information.\n",
    "\n",
    "6. Embedding-based insights:\n",
    "  - When applicable, use similarity scores or semantic reasoning to explain result relevance.\n",
    "  - Explain whether a page/image was returned due to textual match, visual similarity, or both.\n",
    "\n",
    "7. Providing structured output:\n",
    "  For CSV:\n",
    "    CSV Record Summary:\n",
    "    - Dataset: [file_name] (ID: [dataset_id])\n",
    "    - Columns: [col1, col2, ...]\n",
    "    - Record:\n",
    "        col1: value1\n",
    "        col2: value2\n",
    "        ...\n",
    "    - Notes: [Any patterns, anomalies, or insights]\n",
    "\n",
    "  For PDF:\n",
    "    Article Page Match:\n",
    "    - Title: [pdf_title]\n",
    "    - Page: [page_number]\n",
    "    - Summary: [Gemini or page_text extract]\n",
    "    - Notes: [Mention if image, table, or citation was detected]\n",
    "\n",
    "8. Acting responsibly:\n",
    "  - DO NOT make up any values.\n",
    "  - Clearly state when a record, page, or result is not found.\n",
    "  - Always cite retrieved content from PDFs with accurate source info (title, page).\n",
    "  - Respect dataset diversity and content type (CSV vs. image vs. article page).\n",
    "\n",
    "This assistant supports analysts, researchers, and non-technical users in exploring multimodal data and extracting reliable insights from both structured CSVs and academic PDFs, using state-of-the-art embeddings and Gemini-based summarization.\n",
    "\n",
    "DO NOT MAKE UP ANY INFORMATION.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", agent_purpose),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Memory Using MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        MONGODB_URI, session_id, database_name=DB_NAME, collection_name=\"history\"\n",
    "    )\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", chat_memory=get_session_history(\"latest_agent_session\")\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot agent and node\n",
    "toolbox = []\n",
    "\n",
    "# Add tools\n",
    "toolbox.extend(records_embeddings_collection_tools)\n",
    "toolbox.extend(docs_multimodal_collection_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='csv_files_vector_search_tool', description='Perform a vector similarity search on safety procedures.\\n\\nArgs:\\n    query (str): The search query string.\\n    k (int, optional): Number of top results to return. Defaults to 5.\\n\\nReturns:\\n    list: List of tuples (Document, score), where Document is a record\\n          and score is the similarity score (lower is more similar).\\n\\nNote:\\n    Uses the global vector_store_csv_files for the search.', args_schema=<class 'langchain_core.utils.pydantic.csv_files_vector_search_tool'>, func=<function csv_files_vector_search_tool at 0x168a18860>),\n",
       " StructuredTool(name='create_new_record', description=\"Create and validate a new generic CSV record.\\n\\nArgs:\\n    new_record (dict): Dictionary containing a row from a CSV file. \\n                       Must include 'dataset_id' as a key.\\n\\nReturns:\\n    dict: Validated and formatted record document.\\n\\nRaises:\\n    ValueError: If the input data is invalid or incomplete.\\n\\nNote:\\n    Uses Pydantic for data validation via create_generic_record_document function.\", args_schema=<class 'langchain_core.utils.pydantic.create_new_record'>, func=<function create_new_record at 0x16917a5c0>),\n",
       " StructuredTool(name='article_page_vector_search_tool', description='Search academic papers (page-level) using a text query via multimodal embeddings.\\nReturns the top 5 page-level summaries with citations.\\n\\nArgs:\\n    query (str): The textual search query.\\n    model (str): Embedding model (\"sbert\", \"clip\", \"voyage\").\\n    collection (str): MongoDB collection name to search in.\\n\\nReturns:\\n    str: Top 5 matching pages, each with citation and summary.', args_schema=<class 'langchain_core.utils.pydantic.article_page_vector_search_tool'>, func=<function article_page_vector_search_tool at 0x33118b1a0>),\n",
       " StructuredTool(name='vector_search_image_tool', description='Search academic papers using either an image or text query. If text is provided, it generates\\na CLIP embedding from the query and searches against page images. If image is provided, it\\nsearches using image embeddings.\\n\\nArgs:\\n    image_bytes (bytes, optional): Query image content.\\n    text_query (str, optional): Text query to generate image embedding.\\n    collection_name (str): MongoDB collection name.\\n\\nReturns:\\n    str: Top 5 matching pages with citation and Gemini summary.', args_schema=<class 'langchain_core.utils.pydantic.vector_search_image_tool'>, func=<function vector_search_image_tool at 0x1609db7e0>)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(llm, toolbox, prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chatbot agent and node\n",
    "# toolbox = []\n",
    "\n",
    "# # Add tools\n",
    "# toolbox.extend(records_embeddings_collection_tools)\n",
    "# #toolbox.extend(accident_report_collection_tools)\n",
    "\n",
    "# # Create Agent\n",
    "# chatbot_agent = create_agent(\n",
    "#     llm,\n",
    "#     toolbox,\n",
    "#     system_message=\"\"\"\n",
    "# You are an intelligent CSV Dataset Assistant Agent specializing in helping users interact with structured datasets uploaded as CSV files. These datasets vary in schema and come from diverse domains such as healthcare, finance, and operations. You work with pre-indexed records and their vector embeddings, as well as metadata describing the datasets.\n",
    "\n",
    "# Your key responsibilities include:\n",
    "\n",
    "# 1. Searching and retrieving records from datasets:\n",
    "#   - Use the provided search and similarity tools to find relevant records based on natural language queries.\n",
    "#   - Retrieve records with the highest hybrid or semantic similarity scores.\n",
    "#   - Present relevant record content clearly, along with associated dataset metadata such as column names and file name.\n",
    "\n",
    "# 2. Creating new records:\n",
    "#   - When provided with a dictionary representing a CSV row, use the `create_new_record` tool to validate and structure it.\n",
    "#   - Ensure the new record includes a `dataset_id` and any available columns.\n",
    "#   - Construct a combined text representation (`combined_info`) to support future search and embedding operations.\n",
    "\n",
    "# 3. Answering data-related queries:\n",
    "#   - Interpret the content of records and explain what a record or column means.\n",
    "#   - Identify patterns or insights in retrieved results, such as common values, distributions, or outliers.\n",
    "#   - Assist users in comparing records from different datasets, especially when schemas overlap.\n",
    "\n",
    "# 4. Dataset context awareness:\n",
    "#   - Understand that not all datasets have the same column names.\n",
    "#   - Use dataset metadata like `file_name`, `n_columns`, `n_rows`, and `column_types` to answer questions about the dataset.\n",
    "#   - Explain what each dataset represents and how the records are structured.\n",
    "\n",
    "# 5. Embedding-based insights:\n",
    "#   - When relevant, utilize similarity scores or embeddings to explain why certain records were retrieved.\n",
    "#   - Inform users if a record was returned due to semantic similarity rather than exact column match.\n",
    "\n",
    "# 6. Providing structured output:\n",
    "#   When summarizing or comparing records, use the following format:\n",
    "\n",
    "#   CSV Record Summary:\n",
    "#   - Dataset: [file_name] (ID: [dataset_id])\n",
    "#   - Columns: [col1, col2, ...]\n",
    "#   - Record:\n",
    "#       col1: value1\n",
    "#       col2: value2\n",
    "#       ...\n",
    "#   - Notes: [Any patterns, anomalies, or insights]\n",
    "\n",
    "# 7. Creating new entries:\n",
    "#   When creating a new record, ensure you include:\n",
    "#   - All available fields (can vary by dataset)\n",
    "#   - The `dataset_id`\n",
    "#   - A properly formatted `combined_info` string\n",
    "#   - (Optional) A placeholder for `embedding`, if it's not computed yet\n",
    "\n",
    "# 8. Acting responsibly:\n",
    "#   - Do not invent or hallucinate values.\n",
    "#   - If a record or dataset is not found, clearly state that and suggest uploading or creating a new one.\n",
    "#   - Respect the diversity of schemas and don't assume fixed column names.\n",
    "\n",
    "# This assistant is intended to support data analysts, researchers, and non-technical users in navigating and extracting insights from structured tabular data efficiently and reliably.\n",
    "\n",
    "# DO NOT MAKE UP ANY INFORMATION.\n",
    "#     \"\"\",\n",
    "# )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "chatbot_node = functools.partial(\n",
    "    agent_node, agent=agent, name=\"Diabetes Research Assistant Agent(DRAA)\"\n",
    ")\n",
    "tool_node = ToolNode(toolbox, name=\"tools\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflow Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x160005d00>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"chatbot\")\n",
    "workflow.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\", END: END})\n",
    "\n",
    "workflow.add_edge(\"tools\", \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "mongo_client = AsyncIOMotorClient(MONGODB_URI)\n",
    "mongodb_checkpointer = MongoDBSaver(mongo_client, DB_NAME, \"state_store\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=mongodb_checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=toolbox,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Dataset Test Queries\n",
    "(These will invoke the CSV retrieval + creation tools like create_new_record, hybrid similarity, etc.)\n",
    "\n",
    "1. **Search CSV records:**\n",
    "\n",
    "- \"Find patients over 65 with high blood pressure from the dataset.\"\n",
    "\n",
    "- \"Show me the top 3 transactions with the highest total amount from the uploaded finance file.\"\n",
    "\n",
    "2. **Insert a new CSV record:**\n",
    "\n",
    "- \"Add this row to the diabetes dataset: age: 50, bmi: 29.2, glucose: 130, outcome: 1.\"\n",
    "\n",
    "- \"Create a new customer record with name: John Doe, purchase: $450, product: smartwatch.\"\n",
    "\n",
    "3. **Explain or compare CSV content:**\n",
    "\n",
    "- \"What does the glucose column represent in the diabetes file?\"\n",
    "\n",
    "- \"Compare two entries with different 'outcome' values and explain the difference.\"\n",
    "\n",
    "4. **Summarize CSV dataset:**\n",
    "\n",
    "- \"How many rows and columns are in the dataset 'diabetes_data.csv'?\"\n",
    "\n",
    "- \"What types of values are in each column of 'hospital_admissions.csv'?\"\n",
    "\n",
    "\n",
    "### PDF Article Page Summary Queries\n",
    "(These should use article_page_vector_search_tool with text-based embedding search on pages.)\n",
    "\n",
    "1. **Textual search inside PDF pages:**\n",
    "\n",
    "- \"What does the article on 'Extreme Learning Machines' say about classification accuracy?\"\n",
    "\n",
    "- \"Find pages discussing neural networks for diabetes prediction.\"\n",
    "\n",
    "- \"Show me summaries of PDF pages that mention performance benchmarks.\"\n",
    "\n",
    "2. **Explain results and source:**\n",
    "\n",
    "- \"Which paper and page mention using support vector machines for feature selection?\"\n",
    "\n",
    "- \"From which page is this quote taken: 'The training speed of ELM is very fast'?\"\n",
    "\n",
    "3. **Cross-modal reference:**\n",
    "\n",
    "- \"Is there any overlap between the CSV dataset and the concepts in the 'Early_Stage_Diabetes_Prediction' paper?\"\n",
    "\n",
    "### Image-Based PDF Search Queries (CLIP Embeddings)\n",
    "(These activate vector_search_image_tool, searching using image bytes.)\n",
    "\n",
    "1. **Search by image:**\n",
    "\n",
    "- \"Find pages with diagrams similar to this [upload chart or graph image].\"\n",
    "\n",
    "- \"I have this image of a medical flowchart ‚Äî which papers contain something similar?\"\n",
    "\n",
    "2. **Understand visual layout:**\n",
    "\n",
    "- \"Which PDF pages contain large tables or heatmaps?\"\n",
    "\n",
    "- \"Upload this chart and tell me which article and page it's from.\"\n",
    "\n",
    "3. **Cross-validation:**\n",
    "\n",
    "- \"Does this image match any pages in the article titled 'Early_Stage_Diabetes_Prediction_via_Extreme_Learning_Machine'?\"\n",
    "\n",
    "- \"Show the summary of the page that matches this image.\"\n",
    "\n",
    "### Multimodal Cross-Checks & Edge Cases\n",
    "1. **Join CSV and PDF insights:**\n",
    "\n",
    "- \"Compare the patient outcomes in the CSV file with the results discussed in the ELM paper.\"\n",
    "\n",
    "- \"Is there a correlation between the glucose levels in the CSV and findings in any PDF pages?\"\n",
    "\n",
    "2. **Cache check validation (Gemini summaries):**\n",
    "\n",
    "- \"Summarize the first page of the PDF titled 'Early_Stage_Diabetes_Prediction_via_Extreme_Learning_Machine'.\"\n",
    "\n",
    "- \"Is there a Gemini-generated summary for page 3 of the diabetes prediction paper?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_gcs(gcs_bucket, key: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Download image bytes from GCS.\n",
    "\n",
    "    Args:\n",
    "        gcs_bucket: GCS bucket instance.\n",
    "        key (str): Blob key in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        bytes: Image bytes.\n",
    "    \"\"\"\n",
    "    blob = gcs_bucket.blob(key)\n",
    "    return blob.download_as_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Äú‚Äù\n",
    "Find articles showing visual evidence of pancreas damage in diabetes.\n",
    "‚ÄúSummarize full papers mentioning HbA1c monitoring.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `article_page_vector_search_tool` with `{'query': 'Machine Learning Methods on the treatment of Diabetes'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management, Page 197]: ```json\n",
      "{\n",
      "  \"summary\": \"This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management when making predictions. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\",\n",
      "  \"mentions\": [\n",
      "    \"diabetes prediction\",\n",
      "    \"diabetes care management\",\n",
      "    \"prediction of diabetes complications\",\n",
      "    \"model calibration\",\n",
      "    \"optimization methods\",\n",
      "    \"model evaluation\"\n",
      "  ],\n",
      "  \"linked_articles\": [\n",
      "    {\n",
      "      \"title\": \"Machine Learning and Data Mining Methods in Diabetes Research\",\n",
      "      \"url\": \"https://doi.org/10.1016/j.csbj.2016.12.005\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"IDF Diabetes Atlas 9th edition 2019\",\n",
      "      \"url\": \"https://diabetesatlas.org/en/\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Non-invasive detection of fasting blood glucose level via electrochemical measurement of saliva\",\n",
      "      \"url\": \"https://doi.org/10.1186/s40064-016-2339-6\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Machine learning approaches for discerning intercorrelation of hematological parameters and glucose level for identification of diabetes mellitus\",\n",
      "      \"url\": \"https://doi.org/10.17877/DE290R-7572\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Prediction of fasting plasma glucose status using anthropometric measures for diagnosing Type 2 diabetes\",\n",
      "      \"url\": \"https://doi.org/10.1109/JBHI.2013.2264509\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Blood Glucose Prediction Using Artificial Neural Networks Trained with the AIDA Diabetes Simulator: A Proof-of-Concept Pilot Study\",\n",
      "      \"url\": \"https://doi.org/10.1155/2011/681786\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Probability Machines: Consistent probability estimation using nonparametric learning machines\",\n",
      "      \"url\": \"https://doi.org/10.3414/ME00-01-0052\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Type 2 diabetes mellitus trajectories and associated risks\",\n",
      "      \"url\": \"https://doi.org/10.1089/big.2015.0029\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Type 2 Diabetes Mellitus Screening and Risk Factors Using Decision Tree: Results of Data Mining\",\n",
      "      \"url\": \"https://doi.org/10.5539/gjhs.v7n5p304\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Comparison of three data mining models for predicting diabetes or prediabetes by risk factors\",\n",
      "      \"url\": \"https://doi.org/10.1016/j.kjms.2012.08.016\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "[Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management, Page 1]: ```json\n",
      "{\n",
      "  \"summary\": \"This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\",\n",
      "  \"mentions\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Type 2 Diabetes\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers, Page 3]: ```json\n",
      "{\n",
      "  \"summary\": \"This research paper focuses on using machine learning to predict diabetes early. It highlights the increasing prevalence of diabetes and cardiovascular diseases, particularly among Canadian and American populations, and emphasizes the importance of early detection. The study involves data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models to identify key features and determine the most effective model for diabetes prediction. The paper reviews existing literature, including studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which utilize various machine learning algorithms and datasets to predict diabetes. The ultimate goal is to improve patient diagnosis and provide clinically useful results.\",\n",
      "  \"mentions\": [\n",
      "    \"machine learning\",\n",
      "    \"data conversion\",\n",
      "    \"association rule mining\",\n",
      "    \"data preprocessing\",\n",
      "    \"Tripathi et al.\",\n",
      "    \"LDA\",\n",
      "    \"KNN\",\n",
      "    \"SVM\",\n",
      "    \"Random Forest (RF)\",\n",
      "    \"Pima Indian Diabetes Database (PIDD)\",\n",
      "    \"Alaa Khaleel et al.\",\n",
      "    \"LR\",\n",
      "    \"NB\",\n",
      "    \"Zou et al.\",\n",
      "    \"decision trees\",\n",
      "    \"neural networks\",\n",
      "    \"MRMR\",\n",
      "    \"PCA\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes, Page 1]: ```json\n",
      "{\n",
      "  \"summary\": \"This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\",\n",
      "  \"mentions\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Pima Indians Diabetes dataset\",\n",
      "    \"Logistic Regression\",\n",
      "    \"Decision Tree\",\n",
      "    \"Random Forest\",\n",
      "    \"k-Nearest Neighbors\",\n",
      "    \"Naive Bayes\",\n",
      "    \"Support Vector Machine\",\n",
      "    \"Gradient Boosting\",\n",
      "    \"Neural Network\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection, Page 14]: ```json\n",
      "{\n",
      "  \"summary\": \"This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\",\n",
      "  \"mentions\": [\n",
      "    \"Machine learning models\",\n",
      "    \"Machine learning algorithms\",\n",
      "    \"Symbiotic organisms search algorithm\",\n",
      "    \"Thermogram Images\"\n",
      "  ],\n",
      "  \"linked_articles\": [\n",
      "    {\n",
      "      \"title\": \"Kaggle publicly available dataset\",\n",
      "      \"url\": \"https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3mArticle Page Match:\n",
      "- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\n",
      "- Page: 197\n",
      "- Summary: This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management when making predictions. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\n",
      "- Notes: Mentions diabetes prediction, diabetes care management, prediction of diabetes complications, model calibration, optimization methods, and model evaluation.\n",
      "\n",
      "Article Page Match:\n",
      "- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\n",
      "- Page: 1\n",
      "- Summary: This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\n",
      "- Notes: Mentions Machine Learning and Type 2 Diabetes.\n",
      "\n",
      "Article Page Match:\n",
      "- Title: Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers\n",
      "- Page: 3\n",
      "- Summary: This research paper focuses on using machine learning to predict diabetes early. It highlights the increasing prevalence of diabetes and cardiovascular diseases, particularly among Canadian and American populations, and emphasizes the importance of early detection. The study involves data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models to identify key features and determine the most effective model for diabetes prediction. The paper reviews existing literature, including studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which utilize various machine learning algorithms and datasets to predict diabetes. The ultimate goal is to improve patient diagnosis and provide clinically useful results.\n",
      "- Notes: Mentions machine learning, data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models.\n",
      "\n",
      "Article Page Match:\n",
      "- Title: A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes\n",
      "- Page: 1\n",
      "- Summary: This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\n",
      "- Notes: Mentions several machine learning techniques including Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network.\n",
      "\n",
      "Article Page Match:\n",
      "- Title: DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection\n",
      "- Page: 14\n",
      "- Summary: This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\n",
      "- Notes: Mentions Machine learning models and algorithms.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get me a list of research papers on the topic Machine Learning Methods on the treatment of Diabetes.',\n",
       " 'chat_history': \"Human: Find similar records to this one: Age: 45, Gender: Male, Polyuria: Yes\\nAI: The search results returned records from the dataset with `dataset_id`: `685052b612c7b37a8d059b93`.\\n\\nThe records share some similarities with your input, but the columns are different. The dataset contains information about diabetes patients, with columns like 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', and 'Outcome'.\\n\\nThe records retrieved have ages close to 45 (37, 47, 53, 63, 37). However, the search was based on semantic similarity, as the dataset does not contain 'Gender' or 'Polyuria' columns.\\n\\nHuman: Get me a list of research papers on the topic Machine Learning Methods on the treatment of Diabetes.\\nAI: Here are some research papers that discuss machine learning methods for the treatment of diabetes:\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 197\\n- Summary: This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management in prediction models. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\\n- Notes: Mentions diabetes prediction, diabetes care management, prediction models, and model evaluation.\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 1\\n- Summary: This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\\n- Notes: Mentions Machine Learning and Type 2 Diabetes.\\n\\nArticle Page Match:\\n- Title: Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers\\n- Page: 3\\n- Summary: This research paper focuses on using machine learning to predict diabetes early, particularly in Canadian patients at risk. It outlines the study\\\\'s objectives, including data conversion, association rule mining, data preprocessing, applying and comparing different machine learning models, and analyzing the best model\\\\'s performance against existing research. The literature review section discusses previous studies that used diabetes datasets and machine learning algorithms to predict diabetes, highlighting the methodologies and results of those investigations. The paper references studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which employed various machine learning techniques like LDA, KNN, SVM, Random Forest, LR, and NB on datasets like the Pima Indian Diabetes Database to predict diabetes with varying degrees of accuracy.\\n- Notes: Mentions various machine learning techniques and datasets used for diabetes prediction.\\n\\nArticle Page Match:\\n- Title: A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes\\n- Page: 1\\n- Summary: This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\\n- Notes: Discusses the performance of different machine learning techniques on the Pima Indians Diabetes dataset.\\n\\nArticle Page Match:\\n- Title: DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection\\n- Page: 14\\n- Summary: This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\\n- Notes: Mentions the application of machine learning models and algorithms for diabetes prediction and diagnosis.\\n\\nHuman: Find articles showing visual evidence of pancreas damage in diabetes.\\nAI: Could you please provide the image file content (in bytes) that you want to use to search for visual evidence of pancreas damage in diabetes? I need the image data to perform the search. Also, please specify the collection name.\\n\\nHuman: Find articles showing visual evidence of pancreas damage in diabetes. Check the images in your database\\nAI: To search for visual evidence of pancreas damage in diabetes, I need to use the `vector_search_image_tool`. I don't have access to local files, so I need you to first provide me with the image file content as a string. Once you provide the image, I will use it to search for similar images in the academic papers. I also need the collection name.\\n\",\n",
       " 'output': 'Article Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 197\\n- Summary: This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management when making predictions. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\\n- Notes: Mentions diabetes prediction, diabetes care management, prediction of diabetes complications, model calibration, optimization methods, and model evaluation.\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 1\\n- Summary: This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\\n- Notes: Mentions Machine Learning and Type 2 Diabetes.\\n\\nArticle Page Match:\\n- Title: Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers\\n- Page: 3\\n- Summary: This research paper focuses on using machine learning to predict diabetes early. It highlights the increasing prevalence of diabetes and cardiovascular diseases, particularly among Canadian and American populations, and emphasizes the importance of early detection. The study involves data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models to identify key features and determine the most effective model for diabetes prediction. The paper reviews existing literature, including studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which utilize various machine learning algorithms and datasets to predict diabetes. The ultimate goal is to improve patient diagnosis and provide clinically useful results.\\n- Notes: Mentions machine learning, data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models.\\n\\nArticle Page Match:\\n- Title: A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes\\n- Page: 1\\n- Summary: This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\\n- Notes: Mentions several machine learning techniques including Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network.\\n\\nArticle Page Match:\\n- Title: DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection\\n- Page: 14\\n- Summary: This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\\n- Notes: Mentions Machine learning models and algorithms.\\n'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Get me a list of research papers on the topic Machine Learning Methods on the treatment of Diabetes.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `article_page_vector_search_tool` with `{'model': 'clip', 'query': 'Correlation Between Diabetes and Related Chronic Conditions'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[Determination_of_personalized_diabetes_treatment_plans_using_a_two-delay_model, Page 7]: ```json\n",
      "{\n",
      "  \"summary\": \"This section discusses a mathematical model for blood glucose regulation, focusing on Type II diabetes. It uses an eigenvalue equation to analyze the stability of blood glucose oscillations. The model incorporates parameters such as insulin sensitivity (Œ≥), pancreatic efficiency (Œ≤), and glucose uptake efficiency (Œ∫). The analysis aims to identify parameter values that result in healthy blood glucose profiles, characterized by stable oscillations within a moderate range (80-120 mg/dl). The section also explores how changes in Œ≤ and Œ≥ affect the eigenvalues and, consequently, the stability of glucose oscillations.\",\n",
      "  \"mentions\": [\n",
      "    \"Type II diabetes\",\n",
      "    \"eigenvalue equation\",\n",
      "    \"insulin sensitivity (Œ≥)\",\n",
      "    \"pancreatic efficiency (Œ≤)\",\n",
      "    \"glucose uptake efficiency (Œ∫)\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[Deep_Learning_vs._Human_Graders_for_Classifying_Severity_Levels_of_Diabetic_Retinopathy_in_a_Real-World_Nationwide_Screening_Program, Page 6]: ```json\n",
      "{\n",
      "  \"summary\": \"This section discusses the methodology used in the study. It details the estimation of the sample size required, which was determined to be at least 7,450 patients with diabetes, accounting for a 20% rate of ungradable images. The development of a deep learning algorithm for predicting Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) is also described, referencing Krause et al. (2). The algorithm utilizes a convolutional neural network with an \\\"Inception-v4\\\" architecture (12) to predict DR grade, referable DME, gradability, and image quality. The model was trained using fundus images and adjusted its parameters to increase accuracy, with an ensemble of ten individual models created for the final output.\",\n",
      "  \"mentions\": [\n",
      "    \"Deep learning algorithm\",\n",
      "    \"Diabetic Retinopathy (DR)\",\n",
      "    \"Diabetic Macular Edema (DME)\",\n",
      "    \"convolutional neural network\",\n",
      "    \"Inception-v4 architecture\",\n",
      "    \"ensemble of ten individual models\"\n",
      "  ],\n",
      "  \"linked_articles\": [\n",
      "    {\n",
      "      \"title\": \"Krause et al.\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Inception-v4\",\n",
      "      \"url\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "[HealthEdge:_A_Machine_Learning-Based_Smart_Healthcare_Framework_for_Prediction_of_Type_2_Diabetes_in_an_Integrated_IoT,_Edge,_and_Cloud_Computing_System, Page 2]: ```json\n",
      "{\n",
      "  \"summary\": \"This paper introduces HealthEdge, a machine learning-based smart healthcare framework designed for predicting type 2 diabetes within an integrated IoT-edge-cloud computing system. The system uses medical sensors and devices to analyze diabetes risk factors and predict the likelihood of diabetes in individuals. The machine learning model is trained in the cloud and deployed on edge servers for prediction. The paper discusses the implementation steps and evaluates the system's performance using real-life diabetes datasets and common machine learning algorithms. It also reviews related work on machine learning-based diabetes prediction and concludes with potential future research directions.\",\n",
      "  \"mentions\": [\n",
      "    \"HealthEdge\",\n",
      "    \"Logistic Regression (LR)\",\n",
      "    \"Support Vector Machine (SVM)\",\n",
      "    \"Decision Tree (DT)\",\n",
      "    \"Random Forest (RF)\",\n",
      "    \"Ensemble Majority Voting (EMV)\",\n",
      "    \"Regularized Generalized Linear Model (Glmnet)\",\n",
      "    \"Extreme Gradient Boosting (XGBoost)\",\n",
      "    \"Gradient Boosting Machine (GBM)\",\n",
      "    \"Bayes Point Machine (BPM)\",\n",
      "    \"Average Perceptron (AP)\",\n",
      "    \"Decision Forest (DF)\",\n",
      "    \"Locally Deep SVM (LD-SVM)\",\n",
      "    \"Decision Jungle (DJ)\",\n",
      "    \"Neural Network (NN)\",\n",
      "    \"Na√Øve Bayes (NB)\",\n",
      "    \"Classification and Regression Tree (CART)\",\n",
      "    \"k Nearest Neighbor (kNN)\",\n",
      "    \"NHANES (National Health and Nutrition Examination Survey)\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[A_Multinomial_Model_for_Comorbidity_in_England_of_Longstanding_CVD,_Diabetes,_and_Obesity, Page 3]: ```json\n",
      "{\n",
      "  \"summary\": \"The text discusses the relationship between obesity, diabetes, and cardiovascular disease (CVD), noting that obesity is a major factor in the growth of diabetes, and CVD is a leading cause of death for people with diabetes. It critiques the existing comorbidity research for primarily focusing on the elderly, neglecting the broader multifaceted processes influencing individual health. The text advocates for the multifactorial model of disease causation, which considers socio-economic circumstances, demographic, and genetic factors as key determinants of individual health outcomes. It also highlights the influence of socioeconomic status on health outcomes throughout an individual's life course, affecting childhood nutrition, behaviors, and illness.\",\n",
      "  \"mentions\": [\n",
      "    \"multifactorial model of disease causation\"\n",
      "  ],\n",
      "  \"linked_articles\": []\n",
      "}\n",
      "```\n",
      "\n",
      "[Bayesian_modelling_for_spatially_misaligned_health_areal_data:_a_multiple_membership_approach, Page 3]: ```json\n",
      "{\n",
      "  \"summary\": \"This paper addresses the underreporting of diabetes as a cause of death and introduces a methodological approach that incorporates a multiple membership approach into a spatial prior. It also presents a computational contribution through the implementation of a bivariate CAR spatial prior into Rstan, a probabilistic programming platform using Hamiltonian Monte Carlo (HMC). The study applies these methods to diabetes prevalence and mortality data, embedding causal information by using the GMCAR prior to estimate spatial random effects for mortality conditional on prevalence. The paper is structured to describe the dataset, review conditional autoregressive priors, incorporate the multiple membership approach into a spatial prior, discuss the implementation in Stan, and present a simulation study and findings on diabetes prevalence and mortality.\",\n",
      "  \"mentions\": [\n",
      "    \"multiple membership approach\",\n",
      "    \"spatial prior\",\n",
      "    \"bivariate CAR spatial prior\",\n",
      "    \"Rstan\",\n",
      "    \"Hamiltonian Monte Carlo (HMC)\",\n",
      "    \"GMCAR prior\"\n",
      "  ],\n",
      "  \"linked_articles\": [\n",
      "    {\n",
      "      \"title\": \"Stokes et al. (2017)\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Carpenter et al., 2017\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Stan Development Team, 2018\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Morris et al., 2019\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Joseph, 2016\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Jin et al., 2005\",\n",
      "      \"url\": null\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Schnell et al., 2019\",\n",
      "      \"url\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3mHere are some articles that discuss the correlation between diabetes and related chronic conditions, based on the visual content of their pages:\n",
      "\n",
      "*   **Determination\\_of\\_personalized\\_diabetes\\_treatment\\_plans\\_using\\_a\\_two-delay\\_model, Page 7:** This page presents a mathematical model for blood glucose regulation in Type II diabetes, including factors like insulin sensitivity and glucose uptake efficiency.\n",
      "*   **Deep\\_Learning\\_vs.\\_Human\\_Graders\\_for\\_Classifying\\_Severity\\_Levels\\_of\\_Diabetic\\_Retinopathy\\_in\\_a\\_Real-World\\_Nationwide\\_Screening\\_Program, Page 6:** The page discusses a deep learning algorithm for predicting Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) using convolutional neural networks.\n",
      "*   **HealthEdge:\\_A\\_Machine\\_Learning-Based\\_Smart\\_Healthcare\\_Framework\\_for\\_Prediction\\_of\\_Type\\_2\\_Diabetes\\_in\\_an\\_Integrated\\_IoT,\\_Edge,\\_and\\_Cloud\\_Computing\\_System, Page 2:** This paper introduces a machine learning framework (HealthEdge) for predicting type 2 diabetes using IoT and cloud computing.\n",
      "*   **A\\_Multinomial\\_Model\\_for\\_Comorbidity\\_in\\_England\\_of\\_Longstanding\\_CVD,\\_Diabetes,\\_and\\_Obesity, Page 3:** This page highlights the relationship between obesity, diabetes, and cardiovascular disease (CVD), emphasizing that obesity is a major factor in diabetes and CVD is a leading cause of death for people with diabetes.\n",
      "*   **Bayesian\\_modelling\\_for\\_spatially\\_misaligned\\_health\\_areal\\_data:\\_a\\_multiple\\_membership\\_approach, Page 3:** This paper introduces a method to address the underreporting of diabetes as a cause of death, using a multiple membership approach within a spatial prior.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Find articles showing visuals of Correlation Between Diabetes and Related Chronic Conditions.',\n",
       " 'chat_history': 'Human: Find similar records to this one: Age: 45, Gender: Male, Polyuria: Yes\\nAI: The search results returned records from the dataset with `dataset_id`: `685052b612c7b37a8d059b93`.\\n\\nThe records share some similarities with your input, but the columns are different. The dataset contains information about diabetes patients, with columns like \\'Pregnancies\\', \\'Glucose\\', \\'BloodPressure\\', \\'SkinThickness\\', \\'Insulin\\', \\'BMI\\', \\'DiabetesPedigreeFunction\\', \\'Age\\', and \\'Outcome\\'.\\n\\nThe records retrieved have ages close to 45 (37, 47, 53, 63, 37). However, the search was based on semantic similarity, as the dataset does not contain \\'Gender\\' or \\'Polyuria\\' columns.\\n\\nHuman: Get me a list of research papers on the topic Machine Learning Methods on the treatment of Diabetes.\\nAI: Here are some research papers that discuss machine learning methods for the treatment of diabetes:\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 197\\n- Summary: This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management in prediction models. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\\n- Notes: Mentions diabetes prediction, diabetes care management, prediction models, and model evaluation.\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 1\\n- Summary: This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\\n- Notes: Mentions Machine Learning and Type 2 Diabetes.\\n\\nArticle Page Match:\\n- Title: Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers\\n- Page: 3\\n- Summary: This research paper focuses on using machine learning to predict diabetes early, particularly in Canadian patients at risk. It outlines the study\\\\\\'s objectives, including data conversion, association rule mining, data preprocessing, applying and comparing different machine learning models, and analyzing the best model\\\\\\'s performance against existing research. The literature review section discusses previous studies that used diabetes datasets and machine learning algorithms to predict diabetes, highlighting the methodologies and results of those investigations. The paper references studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which employed various machine learning techniques like LDA, KNN, SVM, Random Forest, LR, and NB on datasets like the Pima Indian Diabetes Database to predict diabetes with varying degrees of accuracy.\\n- Notes: Mentions various machine learning techniques and datasets used for diabetes prediction.\\n\\nArticle Page Match:\\n- Title: A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes\\n- Page: 1\\n- Summary: This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\\n- Notes: Discusses the performance of different machine learning techniques on the Pima Indians Diabetes dataset.\\n\\nArticle Page Match:\\n- Title: DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection\\n- Page: 14\\n- Summary: This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\\n- Notes: Mentions the application of machine learning models and algorithms for diabetes prediction and diagnosis.\\n\\nHuman: Find articles showing visual evidence of pancreas damage in diabetes.\\nAI: Could you please provide the image file content (in bytes) that you want to use to search for visual evidence of pancreas damage in diabetes? I need the image data to perform the search. Also, please specify the collection name.\\n\\nHuman: Find articles showing visual evidence of pancreas damage in diabetes. Check the images in your database\\nAI: To search for visual evidence of pancreas damage in diabetes, I need to use the `vector_search_image_tool`. I don\\'t have access to local files, so I need you to first provide me with the image file content as a string. Once you provide the image, I will use it to search for similar images in the academic papers. I also need the collection name.\\n\\nHuman: Get me a list of research papers on the topic Machine Learning Methods on the treatment of Diabetes.\\nAI: Article Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 197\\n- Summary: This paper provides an overview of problem settings for diabetes prediction and care management, including a literature survey of various problems related to both. It emphasizes the importance of considering disease management when making predictions. The paper details problem descriptions, feature sets, and experimental results for predicting diabetes complications, model calibration, optimization methods, baselines, model evaluation, and model results. The authors hope these results will serve as a foundation for future machine learning research in diabetes.\\n- Notes: Mentions diabetes prediction, diabetes care management, prediction of diabetes complications, model calibration, optimization methods, and model evaluation.\\n\\nArticle Page Match:\\n- Title: Machine_Learning_Approaches_for_Type_2_Diabetes_Prediction_and_Care_Management\\n- Page: 1\\n- Summary: This page is the title page of a research paper focusing on the application of machine learning approaches for the prediction and management of Type 2 Diabetes. The authors are affiliated with KenSci Inc. and the University of Washington.\\n- Notes: Mentions Machine Learning and Type 2 Diabetes.\\n\\nArticle Page Match:\\n- Title: Predicting_Diabetes_Using_Machine_Learning:_A_Comparative_Study_of_Classifiers\\n- Page: 3\\n- Summary: This research paper focuses on using machine learning to predict diabetes early. It highlights the increasing prevalence of diabetes and cardiovascular diseases, particularly among Canadian and American populations, and emphasizes the importance of early detection. The study involves data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models to identify key features and determine the most effective model for diabetes prediction. The paper reviews existing literature, including studies by Tripathi et al., Alaa Khaleel et al., and Zou et al., which utilize various machine learning algorithms and datasets to predict diabetes. The ultimate goal is to improve patient diagnosis and provide clinically useful results.\\n- Notes: Mentions machine learning, data conversion, association rule mining, data preprocessing, and the application of multiple machine learning models.\\n\\nArticle Page Match:\\n- Title: A_Comparative_Study_of_Machine_Learning_Techniques_for_Early_Prediction_of_Diabetes\\n- Page: 1\\n- Summary: This study evaluates the effectiveness of several machine learning methods for diabetes prediction using the Pima Indians Diabetes dataset, which includes data on 768 patients. The machine learning techniques assessed include Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network. The Neural Network algorithm performed the best with an accuracy of 78.57 percent, followed by the Random Forest method with an accuracy of 76.30 percent. The study concludes that machine learning algorithms can aid diabetes prediction and be an efficient early detection tool.\\n- Notes: Mentions several machine learning techniques including Logistic Regression, Decision Tree, Random Forest, k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting, and Neural Network.\\n\\nArticle Page Match:\\n- Title: DiabML:_AI-assisted_diabetes_diagnosis_method_with_meta-heuristic-based_feature_selection\\n- Page: 14\\n- Summary: This page contains a list of references used in the research paper. These references are primarily focused on the application of machine learning models and algorithms for diabetes prediction, diagnosis, and related complications. The studies cover various aspects such as early prediction using health indicators, risk prediction, and severity classification of diabetic foot complications using thermogram images.\\n- Notes: Mentions Machine learning models and algorithms.\\n\\nHuman: Find articles showing visual evidence of pancreas damage in diabetes.\\nAI: I was not able to find any articles containing visual evidence of pancreas damage in diabetes using the available tools. I used the text query \"pancreas damage in diabetes\" to search for relevant images in the academic papers. It\\'s possible that the articles in the collection do not contain the specific images I was looking for, or that the image embeddings do not closely match the query.\\n',\n",
       " 'output': 'Here are some articles that discuss the correlation between diabetes and related chronic conditions, based on the visual content of their pages:\\n\\n*   **Determination\\\\_of\\\\_personalized\\\\_diabetes\\\\_treatment\\\\_plans\\\\_using\\\\_a\\\\_two-delay\\\\_model, Page 7:** This page presents a mathematical model for blood glucose regulation in Type II diabetes, including factors like insulin sensitivity and glucose uptake efficiency.\\n*   **Deep\\\\_Learning\\\\_vs.\\\\_Human\\\\_Graders\\\\_for\\\\_Classifying\\\\_Severity\\\\_Levels\\\\_of\\\\_Diabetic\\\\_Retinopathy\\\\_in\\\\_a\\\\_Real-World\\\\_Nationwide\\\\_Screening\\\\_Program, Page 6:** The page discusses a deep learning algorithm for predicting Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) using convolutional neural networks.\\n*   **HealthEdge:\\\\_A\\\\_Machine\\\\_Learning-Based\\\\_Smart\\\\_Healthcare\\\\_Framework\\\\_for\\\\_Prediction\\\\_of\\\\_Type\\\\_2\\\\_Diabetes\\\\_in\\\\_an\\\\_Integrated\\\\_IoT,\\\\_Edge,\\\\_and\\\\_Cloud\\\\_Computing\\\\_System, Page 2:** This paper introduces a machine learning framework (HealthEdge) for predicting type 2 diabetes using IoT and cloud computing.\\n*   **A\\\\_Multinomial\\\\_Model\\\\_for\\\\_Comorbidity\\\\_in\\\\_England\\\\_of\\\\_Longstanding\\\\_CVD,\\\\_Diabetes,\\\\_and\\\\_Obesity, Page 3:** This page highlights the relationship between obesity, diabetes, and cardiovascular disease (CVD), emphasizing that obesity is a major factor in diabetes and CVD is a leading cause of death for people with diabetes.\\n*   **Bayesian\\\\_modelling\\\\_for\\\\_spatially\\\\_misaligned\\\\_health\\\\_areal\\\\_data:\\\\_a\\\\_multiple\\\\_membership\\\\_approach, Page 3:** This paper introduces a method to address the underreporting of diabetes as a cause of death, using a multiple membership approach within a spatial prior.'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Find articles showing visuals of Correlation Between Diabetes and Related Chronic Conditions.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `csv_files_vector_search_tool` with `{'query': 'Age: 45, Gender: Male, Polyuria: Yes'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[(Document(id='685056e712c7b37a8d059cdb', metadata={'_id': '685056e712c7b37a8d059cdb', 'Pregnancies': 10, 'Glucose': 179, 'BloodPressure': 70, 'SkinThickness': 0, 'Insulin': 0, 'BMI': 35.1, 'DiabetesPedigreeFunction': 0.2, 'Age': 37, 'Outcome': 0, 'dataset_id': '685052b612c7b37a8d059b93'}, page_content='Pregnancies: 10.0 Glucose: 179.0 Bloodpressure: 70.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 35.1 Diabetespedigreefunction: 0.2 Age: 37.0 Outcome: 0.0'), 0.9143085479736328), (Document(id='685056e712c7b37a8d059c46', metadata={'_id': '685056e712c7b37a8d059c46', 'Pregnancies': 5, 'Glucose': 143, 'BloodPressure': 78, 'SkinThickness': 0, 'Insulin': 0, 'BMI': 45.0, 'DiabetesPedigreeFunction': 0.19, 'Age': 47, 'Outcome': 0, 'dataset_id': '685052b612c7b37a8d059b93'}, page_content='Pregnancies: 5.0 Glucose: 143.0 Bloodpressure: 78.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 45.0 Diabetespedigreefunction: 0.19 Age: 47.0 Outcome: 0.0'), 0.9139258861541748), (Document(id='685058e512c7b37a8d059e72', metadata={'_id': '685058e512c7b37a8d059e72', 'Pregnancies': 2, 'Glucose': 105, 'BloodPressure': 75, 'SkinThickness': 0, 'Insulin': 0, 'BMI': 23.3, 'DiabetesPedigreeFunction': 0.56, 'Age': 53, 'Outcome': 0, 'dataset_id': '685052b612c7b37a8d059b93'}, page_content='Pregnancies: 2.0 Glucose: 105.0 Bloodpressure: 75.0 Skinthickness: 0.0 Insulin: 0.0 Bmi: 23.3 Diabetespedigreefunction: 0.56 Age: 53.0 Outcome: 0.0'), 0.9132981300354004), (Document(id='685056e712c7b37a8d059c9b', metadata={'_id': '685056e712c7b37a8d059c9b', 'Pregnancies': 3, 'Glucose': 142, 'BloodPressure': 80, 'SkinThickness': 15, 'Insulin': 0, 'BMI': 32.4, 'DiabetesPedigreeFunction': 0.2, 'Age': 63, 'Outcome': 0, 'dataset_id': '685052b612c7b37a8d059b93'}, page_content='Pregnancies: 3.0 Glucose: 142.0 Bloodpressure: 80.0 Skinthickness: 15.0 Insulin: 0.0 Bmi: 32.4 Diabetespedigreefunction: 0.2 Age: 63.0 Outcome: 0.0'), 0.9130156636238098), (Document(id='685058e512c7b37a8d059db9', metadata={'_id': '685058e512c7b37a8d059db9', 'Pregnancies': 4, 'Glucose': 189, 'BloodPressure': 110, 'SkinThickness': 31, 'Insulin': 0, 'BMI': 28.5, 'DiabetesPedigreeFunction': 0.68, 'Age': 37, 'Outcome': 0, 'dataset_id': '685052b612c7b37a8d059b93'}, page_content='Pregnancies: 4.0 Glucose: 189.0 Bloodpressure: 110.0 Skinthickness: 31.0 Insulin: 0.0 Bmi: 28.5 Diabetespedigreefunction: 0.68 Age: 37.0 Outcome: 0.0'), 0.9130118489265442)]\u001b[0m\u001b[32;1m\u001b[1;3mThe search results returned records from the dataset with `dataset_id`: `685052b612c7b37a8d059b93`.\n",
      "\n",
      "The records share some similarities with your input, but the columns are different. The dataset contains information about diabetes patients, with columns like 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', and 'Outcome'.\n",
      "\n",
      "The records retrieved have ages close to 45 (37, 47, 53, 63, 37). However, the search was based on semantic similarity, as the dataset does not contain 'Gender' or 'Polyuria' columns.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Find similar records to this one: Age: 45, Gender: Male, Polyuria: Yes',\n",
       " 'chat_history': '',\n",
       " 'output': \"The search results returned records from the dataset with `dataset_id`: `685052b612c7b37a8d059b93`.\\n\\nThe records share some similarities with your input, but the columns are different. The dataset contains information about diabetes patients, with columns like 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', and 'Outcome'.\\n\\nThe records retrieved have ages close to 45 (37, 47, 53, 63, 37). However, the search was based on semantic similarity, as the dataset does not contain 'Gender' or 'Polyuria' columns.\\n\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Find similar records to this one: Age: 45, Gender: Male, Polyuria: Yes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"Sanitize the name to match the pattern '^[a-zA-Z0-9_-]+$'.\"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import warnings\n",
    "\n",
    "from langchain_core.messages import HumanMessage,AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "async def chat_loop():\n",
    "    config = {\"configurable\": {\"thread_id\": \"0\"}}\n",
    "\n",
    "    while True:\n",
    "        user_input = await asyncio.get_event_loop().run_in_executor(\n",
    "            None, input, \"User: \"\n",
    "        )\n",
    "        user_input = user_input.strip()\n",
    "\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            print(\"‚ö†Ô∏è  Empty input skipped.\")\n",
    "            continue\n",
    "\n",
    "        sanitized_name = (\n",
    "            sanitize_name(\"Human\") or \"Anonymous\"\n",
    "        )  # Fallback if sanitized name is empty\n",
    "        state = {\"messages\": [HumanMessage(content=user_input, name=sanitized_name)]}\n",
    "\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        max_retries = 3\n",
    "        retry_delay = 1\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                async for chunk in graph.astream(state, config, stream_mode=\"values\"):\n",
    "                    if chunk.get(\"messages\"):\n",
    "                        last_message = chunk[\"messages\"][-1]\n",
    "                        if isinstance(last_message, AIMessage):\n",
    "                            last_message.name = (\n",
    "                                sanitize_name(last_message.name or \"AI\") or \"AI\"\n",
    "                            )\n",
    "                            print(last_message.content, end=\"\", flush=True)\n",
    "                    elif isinstance(last_message, ToolMessage):\n",
    "                        print(f\"\\n[Tool Used: {last_message.name}]\")\n",
    "                        print(f\"Tool Call ID: {last_message.tool_call_id}\")\n",
    "                        print(f\"Content: {last_message.content}\")\n",
    "                        print(\"Assistant: \", end=\"\", flush=True)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"\\nRetrying in {retry_delay} seconds...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                else:\n",
    "                    print(\"‚õî Max retries reached. Exiting this turn.\")\n",
    "\n",
    "        print(\"\\n\")  # New line after the complete response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Empty input skipped.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949b2d52711b44e59b5730857a476d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type your message here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a41bd151174c29abf7f52a67e2dfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5abe82a6c0476cafeeb39e41589a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Replace this with your actual sanitize_name and graph definitions\n",
    "def sanitize_name(name):\n",
    "    return name.strip()\n",
    "\n",
    "# Dummy placeholder for graph.astream\n",
    "async def dummy_astream(state, config, stream_mode):\n",
    "    # Simulate streaming response chunks\n",
    "    yield {\"messages\": [AIMessage(content=\"Hello, how can I help you?\")]}\n",
    "    await asyncio.sleep(0.5)\n",
    "    yield {\"messages\": [AIMessage(content=\" Here's the answer to your question.\")]}\n",
    "    await asyncio.sleep(0.5)\n",
    "\n",
    "graph = type(\"Graph\", (), {\"astream\": dummy_astream})\n",
    "\n",
    "# Widgets for user input and output\n",
    "input_box = widgets.Text(placeholder=\"Type your message here...\")\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "output_area = widgets.Output(layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'auto'})\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"0\"}}\n",
    "\n",
    "async def handle_user_input(user_input):\n",
    "    sanitized_name = sanitize_name(\"Human\") or \"Anonymous\"\n",
    "    state = {\"messages\": [HumanMessage(content=user_input, name=sanitized_name)]}\n",
    "\n",
    "    async def run_chat():\n",
    "        try:\n",
    "            async for chunk in graph.astream(state, config, stream_mode=\"values\"):\n",
    "                if chunk.get(\"messages\"):\n",
    "                    last_message = chunk[\"messages\"][-1]\n",
    "                    if isinstance(last_message, AIMessage):\n",
    "                        last_message.name = sanitize_name(last_message.name or \"AI\") or \"AI\"\n",
    "                        with output_area:\n",
    "                            print(f\"{last_message.name}: {last_message.content}\", end=\"\", flush=True)\n",
    "                    elif isinstance(last_message, ToolMessage):\n",
    "                        with output_area:\n",
    "                            print(f\"\\n[Tool Used: {last_message.name}]\")\n",
    "                            print(f\"Tool Call ID: {last_message.tool_call_id}\")\n",
    "                            print(f\"Content: {last_message.content}\")\n",
    "                            print(\"Assistant: \", end=\"\", flush=True)\n",
    "        except Exception as e:\n",
    "            with output_area:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "\n",
    "    await run_chat()\n",
    "    with output_area:\n",
    "        print(\"\\n\")  # new line after complete response\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    user_input = input_box.value.strip()\n",
    "    if not user_input:\n",
    "        with output_area:\n",
    "            print(\"‚ö†Ô∏è Empty input skipped.\\n\")\n",
    "        return\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        with output_area:\n",
    "            print(\"Goodbye!\")\n",
    "        input_box.disabled = True\n",
    "        send_button.disabled = True\n",
    "        return\n",
    "\n",
    "    with output_area:\n",
    "        print(f\"User: {user_input}\")\n",
    "\n",
    "    input_box.value = \"\"\n",
    "\n",
    "    # Run async chat in background\n",
    "    asyncio.create_task(handle_user_input(user_input))\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(input_box, send_button, output_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Example Use Cases for This Agent\n",
    "- ‚ÄúFind similar records to this one: Age: 45, Gender: Male, Polyuria: Yes‚Äù\n",
    "\n",
    "- ‚ÄúCreate a new record for dataset X with these values...‚Äù\n",
    "\n",
    "- ‚ÄúWhich datasets have columns related to diabetes symptoms?‚Äù\n",
    "\n",
    "- ‚ÄúCompare a record from dataset A with a similar one from dataset B‚Äù\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n",
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search query \"Age: 45, Gender: Male, Polyuria: Yes\" returned the following records:\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685052b612c7b37a8d059b93 (ID: 685056e712c7b37a8d059cdb)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 10.0\n",
      "      Glucose: 179.0\n",
      "      BloodPressure: 70.0\n",
      "      SkinThickness: 0.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 35.1\n",
      "      DiabetesPedigreeFunction: 0.2\n",
      "      Age: 37.0\n",
      "      Outcome: 0.0\n",
      "- Notes: The age is different (37 vs 45) and the columns don't include Gender or Polyuria.\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685052b612c7b37a8d059b93 (ID: 685056e712c7b37a8d059c46)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 5.0\n",
      "      Glucose: 143.0\n",
      "      BloodPressure: 78.0\n",
      "      SkinThickness: 0.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 45.0\n",
      "      DiabetesPedigreeFunction: 0.19\n",
      "      Age: 47.0\n",
      "      Outcome: 0.0\n",
      "- Notes: The age is close (47 vs 45) but the columns don't include Gender or Polyuria.\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685052b612c7b37a8d059b93 (ID: 685058e512c7b37a8d059e72)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 2.0\n",
      "      Glucose: 105.0\n",
      "      BloodPressure: 75.0\n",
      "      SkinThickness: 0.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 23.3\n",
      "      DiabetesPedigreeFunction: 0.56\n",
      "      Age: 53.0\n",
      "      Outcome: 0.0\n",
      "- Notes: The age is different (53 vs 45) and the columns don't include Gender or Polyuria.\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685052b612c7b37a8d059b93 (ID: 685056e712c7b37a8d059c9b)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 3.0\n",
      "      Glucose: 142.0\n",
      "      BloodPressure: 80.0\n",
      "      SkinThickness: 15.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 32.4\n",
      "      DiabetesPedigreeFunction: 0.2\n",
      "      Age: 63.0\n",
      "      Outcome: 0.0\n",
      "- Notes: The age is different (63 vs 45) and the columns don't include Gender or Polyuria.\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685052b612c7b37a8d059b93 (ID: 685058e512c7b37a8d059db9)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 4.0\n",
      "      Glucose: 189.0\n",
      "      BloodPressure: 110.0\n",
      "      SkinThickness: 31.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 28.5\n",
      "      DiabetesPedigreeFunction: 0.68\n",
      "      Age: 37.0\n",
      "      Outcome: 0.0\n",
      "- Notes: The age is different (37 vs 45) and the columns don't include Gender or Polyuria.\n",
      "\n",
      "The dataset with ID '685052b612c7b37a8d059b93' does not contain 'Gender' or 'Polyuria'. The search results are based on the other fields, especially 'Age'."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n",
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1009492521.py:22: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  hybrid_search_result = hybrid_search.get_relevant_documents(query)\n",
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n",
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset with ID '685052b612c7b37a8d059b93' appears to have columns related to diabetes, specifically 'Glucose', but does not have columns for the other symptoms mentioned (polyuria, polydipsia, weight loss, fatigue, blurred vision, slow healing, frequent infections).\n",
      "\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the IDs of the two records you would like me to compare."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID '685052b612c7b37a8d059b93' is a dataset ID, not a record ID. The ID '685056e712c7b37a8d059cdb' is a record ID. Please provide two record IDs to compare."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n",
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
      "/Users/tubakaraca/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1568: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Record Summary:\n",
      "- Dataset: 685000ea12c7b37a8d058b9c (ID: 6850214d12c7b37a8d058d02)\n",
      "- Columns: Diabetes_Status, Age, B_cells_pct_of_Lymphocytes, Transitional_pct_of_B_cells, Naive_pct_of_B_cells, Nonnegclassnegswitched_Memory_pct_of_B_cells, Classnegswitched_Memory_pct_of_B_cells, _7, MNC_pct_of_Leukocytes, Granulocyte_pct_of_Leukocytes, DC_pct_of_MNCs, HLAnegDR_mfi_of_DCs, Myeloid_pct_of_DCs, Plasmacytoid_pct_DCs, CD11cnegCD123neg_pct_of_DC, Monocytes_pct_of_MNCs, HLAnegDR_mfi_of_Monocytes, Classical_pct_of_Monocytes, NonnegClassical_pct_of_Monocytes, NK_pct_of_Lymphocytes, CD56bright_pct_of_NK, CD56dim_pct_of_NK, CD16pos_pct_of_CD56bright_NK, CD123pos_pct_of_MNCs, T_cells_pct_of_Lymphocytes, CD4_pct_of_T_cells, CD38negHLADRneg_pct_of_CD4, CD38negHLADRpos_pct_of_CD4, CD38posHLADRneg_pct_of_CD4, CD38posHLADRpos_pct_of_CD4, Naive_pct_of_CD4, CD38negHLADRneg_pct_of_Naive_CD4, CD38negHLADRpos_pct_of_Naive_CD4, CD38posHLADRneg_pct_of_Naive_CD4, CD38posHLADRpos_pct_of_Naive_CD4, Tem_pct_of_CD4, CD38negHLADRneg_pct_of_CD4_Tem, CD38negHLADRpos_pct_of_CD4_Tem, CD38posHLADRneg_pct_of_CD4_Tem, CD38posHLADRpos_pct_of_CD4_Tem, Tcm_pct_of_CD4, CD38negHLADRneg_pct_of_CD4_Tcm, CD38negHLADRpos_pct_of_CD4_Tcm, CD38posHLADRneg_pct_of_CD4_Tcm, CD38posHLADRpos_pct_of_CD4_Tcm, Temra_pct_of_CD4, CD38negHLADRneg_pct_of_CD4_Temra, CD38negHLADRpos_pct_of_CD4_Temra, CD38posHLADRneg_pct_of_CD4_Temra, CD38posHLADRpos_pct_of_CD4_Temra, CD8_pct_of_T_cells, CD38negHLADRneg_pct_of_CD8, CD38negHLADRpos_pct_of_CD8, CD38posHLADRneg_pct_of_CD8, CD38posHLADRpos_pct_of_CD8, Naive_pct_of_CD8, CD38negHLADRneg_pct_of_Naive_CD8, CD38negHLADRpos_pct_of_Naive_CD8, CD38posHLADRneg_pct_of_Naive_CD8, CD38posHLADRpos_pct_of_Naive_CD8, Tem_pct_of_CD8, CD38negHLADRneg_pct_of_CD8_Tem, CD38negHLADRpos_pct_of_CD8_Tem, CD38posHLADRneg_pct_of_CD8_Tem, CD38posHLADRpos_pct_of_CD8_Tem, Tcm_pct_of_CD8, CD38negHLADRneg_pct_of_CD8_Tcm, CD38negHLADRpos_pct_of_CD8_Tcm, CD38posHLADRneg_pct_of_CD8_Tcm, CD38posHLADRpos_pct_of_CD8_Tcm, Temra_pct_of_CD8, CD38negHLADRneg_pct_of_CD8_Temra, CD38negHLADRpos_pct_of_CD8_Temra, CD38posHLADRneg_pct_of_CD8_Temra, CD38posHLADRpos_pct_of_CD8_Temra, DN_pct_of_T_cells, DP_pct_of_T_cells, CD45ROposCD183negCD196neg_pct_of_CD3posCD4pos, CD45ROposCD183negCD196pos_pct_of_CD3posCD4pos, CD45ROposCD183posCD196neg_pct_of_CD3posCD4pos, CD45ROposCD183posCD196pos_pct_of_CD3posCD4pos, HLADRpos_pct_of_CD4posCD45ROposCD183negCD196neg, HLADRposCD38pos_pct_of_CD4posCD45ROposCD183negCD196neg, HLADRpos_pct_of_CD4posCD45ROposCD183negCD196pos, HLADRposCD38pos_pct_of_CD4posCD45ROposCD183negCD196pos, HLADRpos_pct_of_CD4posCD45ROposCD183posCD196neg, HLADRposCD38pos_pct_of_CD4posCD45ROposCD183posCD196neg, HLADRpos_pct_of_CD4posCD45ROposCD183posCD196pos, HLADRposCD38pos_pct_of_CD4posCD45ROposCD183posCD196pos, CD183loCD196pos_pct_of_CD3posCD4neg, CD183posCD196neg_pct_of_CD3posCD4neg, CD183negCD196neg_pct_of_CD3posCD4neg, HLApos_pct_of_CD4negCD45ROposCD183loCD196pos, HLAposCD38pos_pct_of_CD4negCD45ROposCD183loCD196pos, HLApos_pct_of_CD4negCD45ROposCD183posCD196neg, HLAposCD38pos_pct_of_CD4negCD45ROposCD183posCD196neg, HLApos_pct_of_CD4negCD45ROposCD183negCD196neg, HLAposCD38pos_pct_of_CD4negCD45ROposCD183negCD196neg, CD183neg_pct_of_CD3posCD4negCD45ROneg, CD183lo_pct_of_CD3posCD4negCD45ROneg, CD183hi_pct_of_CD3posCD4negCD45ROneg, Tfh_pct_of_CD4, CD183pos_pct_of_Tfh, Precursor_Tfh_pct_of_CD4, Memory_Tfh_pct_of_CD4, Tfh_pct_of_CD4posCD45RAnegCD185pos, Precursor_Tfh_pct_of_CD4posCD45RAnegCD185pos, Memory_Tfh__pct_of_CD4posCD45RAnegCD185pos, Naive_CD4_CD279mfi, Tem_CD4_CD279mfi, Tcm_CD4_CD279mfi, Temra_CD4_CD279mfi, Temra_CD4_CD279mfi_Rel_Diff, Tcm_CD4_CD279mfi_Rel_Diff, Tem_CD4_CD279mfi_Rel_Diff, CD279pos_pct_of_Naive_CD4, CD279pos_pct_of_CD4_Tem, CD279pos_pct_of_CD4_Tcm, CD279pos_pct_of_CD4_Temra, CD183pos_pct_of_Naive_CD4, CD183pos_pct_of_CD4_Tem, CD183pos_pct_of_CD4_Tcm, CD183pos_pct_of_CD4_Temra, CD185pos_pct_of_Naive_CD4, CD185pos_pct_of_CD4_Tem, CD185pos_pct_of_CD4_Tcm, CD185pos_pct_of_CD4_Temra, Naive_CD8_CD279mfi, Tem_CD8_CD279mfi, Tcm_CD8_CD279mfi, Temra_CD8_CD279mfi, Temra_CD8_CD279mfi_Rel_Diff, Tcm_CD8_CD279mfi_Rel_Diff, Tem_CD8_CD279mfi_Rel_Diff, CD279pos_pct_of_Naive_CD8, CD279pos_pct_of_CD8_Tcm, CD279pos_pct_of_CD8_Tem, CD279pos_pct_of_CD8_Temra, CD183pos_pct_of_Naive_CD8, CD183pos_pct_of_CD8_Tcm, CD183pos_pct_of_CD8_Tem, CD183pos_pct_of_CD8_Temra, CD185pos_pct_of_Naive_CD8, CD185pos_pct_of_CD8_Tcm, CD185pos_pct_of_CD8_Tem, CD185pos_pct_of_CD8_Temra, CD183neg_pctof_CD3posCD4neg_Naive, CD183hi_pct_of_CD3posCD4neg_Naive, CD183lo_pct_of_CD3posCD4neg_Naive, CD183neg_pct_of_CD3posCD4neg_Tem, CD183hi_pct_of_CD3posCD4neg_Tem, CD183lo_pct_of_CD3posCD4neg_Tem, CD183neg_pct_of_CD3posCD4neg_Tcm, CD183hi_pct_of_CD3posCD4neg_Tcm, CD183lo_pct_of_CD3posCD4neg_Tcm, CD183neg_pct_of_CD3posCD4neg_Temra, CD183hi_pct_of_CD3posCD4neg_Temra, CD183lo_pct_of_CD3posCD4neg_Temra, Treg_pct_of_CD4pos, CD45ROpos_pct_of_Treg, CD25mfi_of_CD45ROneg_Treg, CD25mfi_of_CD45ROpos_Treg, CD45ROpos_Treg_CD25mfi_Rel_Diff, CD45ROposHLADRposCCR4pos_pct_of_Treg, CD25mfi_of_CD45ROneg_Tconv, CD25mfi_of_CD45ROpos_Tconv, CD45ROpos_Tconv_CD25mfi_Rel_Diff, CD45ROposCCR4pos_pct_of_Tconv, CD45ROposHLADRposCCR4pos_pct_of_Tconv, CD25mfi_of_CD45ROnegCD3posCD4neg, CD25mfi_of_CD45ROposCD3posCD4neg, CD45ROposCD3posCD4neg_CD25mfi_Rel_Diff, CD45ROposCCR4pos_pct_of_CD3posCD4neg, CD45ROposHLADRposCCR4pos_pct_of_CD3posCD4neg, _174, _175, _176, _177, _178, _179, _180, _181, _182, _183, _184, _185, _186, _187, _188, _189, _190, _191, _192, _193, dataset_id\n",
      "- Record:\n",
      "      Diabetes_status: T1D\n",
      "      Age: 43.13483915\n",
      "      B_cells_pct_of_Lymphocytes: 8.62\n",
      "      Transitional_pct_of_B_cells: 4.44\n",
      "      Naive_pct_of_B_cells: 49.9\n",
      "      Nonnegclassnegswitched_Memory_pct_of_B_cells: 23.3\n",
      "      Classnegswitched_Memory_pct_of_B_cells: 21.8\n",
      "      _7: 0.523\n",
      "      MNC_pct_of_Leukocytes: nan\n",
      "      Granulocyte_pct_of_Leukocytes: nan\n",
      "      DC_pct_of_MNCs: nan\n",
      "      HLAnegDR_mfi_of_DCs: nan\n",
      "      Myeloid_pct_of_DCs: nan\n",
      "      Plasmacytoid_pct_DCs: nan\n",
      "      CD11cnegCD123neg_pct_of_DC: nan\n",
      "      Monocytes_pct_of_MNCs: nan\n",
      "      HLAnegDR_mfi_of_Monocytes: nan\n",
      "      Classical_pct_of_Monocytes: nan\n",
      "      NonnegClassical_pct_of_Monocytes: nan\n",
      "      NK_pct_of_Lymphocytes: nan\n",
      "      CD56bright_pct_of_NK: nan\n",
      "      CD56dim_pct_of_NK: nan\n",
      "      CD16pos_pct_of_CD56bright_NK: nan\n",
      "      CD123pos_pct_of_MNCs: nan\n",
      "      T_cells_pct_of_Lymphocytes: 62.9\n",
      "      CD4_pct_of_T_cells: 54.8\n",
      "      CD38negHLADRneg_pct_of_CD4: 45.5\n",
      "      CD38negHLADRpos_pct_of_CD4: 4.51\n",
      "      CD38posHLADRneg_pct_of_CD4: 48.1\n",
      "      CD38posHLADRpos_pct_of_CD4: 1.89\n",
      "      Naive_pct_of_CD4: 38.5\n",
      "      CD38negHLADRneg_pct_of_Naive_CD4: 4.83\n",
      "      CD38negHLADRpos_pct_of_Naive_CD4: 0.0\n",
      "      CD38posHLADRneg_pct_of_Naive_CD4: 94.0\n",
      "      CD38posHLADRpos_pct_of_Naive_CD4: 1.21\n",
      "      Tem_pct_of_CD4: 11.3\n",
      "      CD38negHLADRneg_pct_of_CD4_Tem: 74.3\n",
      "      CD38negHLADRpos_pct_of_CD4_Tem: 17.5\n",
      "      CD38posHLADRneg_pct_of_CD4_Tem: 4.97\n",
      "      CD38posHLADRpos_pct_of_CD4_Tem: 3.22\n",
      "      Tcm_pct_of_CD4: 50.0\n",
      "      CD38negHLADRneg_pct_of_CD4_Tcm: 70.3\n",
      "      CD38negHLADRpos_pct_of_CD4_Tcm: 5.04\n",
      "      CD38posHLADRneg_pct_of_CD4_Tcm: 22.7\n",
      "      CD38posHLADRpos_pct_of_CD4_Tcm: 1.99\n",
      "      Temra_pct_of_CD4: 0.166\n",
      "      CD38negHLADRneg_pct_of_CD4_Temra: 40.0\n",
      "      CD38negHLADRpos_pct_of_CD4_Temra: 0.0\n",
      "      CD38posHLADRneg_pct_of_CD4_Temra: 20.0\n",
      "      CD38posHLADRpos_pct_of_CD4_Temra: 40.0\n",
      "      CD8_pct_of_T_cells: 39.3\n",
      "      CD38negHLADRneg_pct_of_CD8: 42.7\n",
      "      CD38negHLADRpos_pct_of_CD8: 5.14\n",
      "      CD38posHLADRneg_pct_of_CD8: 44.4\n",
      "      CD38posHLADRpos_pct_of_CD8: 7.73\n",
      "      Naive_pct_of_CD8: 15.6\n",
      "      CD38negHLADRneg_pct_of_Naive_CD8: 8.63\n",
      "      CD38negHLADRpos_pct_of_Naive_CD8: 0.595\n",
      "      CD38posHLADRneg_pct_of_Naive_CD8: 88.1\n",
      "      CD38posHLADRpos_pct_of_Naive_CD8: 2.68\n",
      "      Tem_pct_of_CD8: 48.1\n",
      "      CD38negHLADRneg_pct_of_CD8_Tem: 51.8\n",
      "      CD38negHLADRpos_pct_of_CD8_Tem: 6.74\n",
      "      CD38posHLADRneg_pct_of_CD8_Tem: 32.7\n",
      "      CD38posHLADRpos_pct_of_CD8_Tem: 8.77\n",
      "      Tcm_pct_of_CD8: 29.5\n",
      "      CD38negHLADRneg_pct_of_CD8_Tcm: 43.7\n",
      "      CD38negHLADRpos_pct_of_CD8_Tcm: 3.92\n",
      "      CD38posHLADRneg_pct_of_CD8_Tcm: 43.4\n",
      "      CD38posHLADRpos_pct_of_CD8_Tcm: 8.93\n",
      "      Temra_pct_of_CD8: 6.85\n",
      "      CD38negHLADRneg_pct_of_CD8_Temra: 52.0\n",
      "      CD38negHLADRpos_pct_of_CD8_Temra: 9.46\n",
      "      CD38posHLADRneg_pct_of_CD8_Temra: 31.8\n",
      "      CD38posHLADRpos_pct_of_CD8_Temra: 6.76\n",
      "      DN_pct_of_T_cells: 5.31\n",
      "      DP_pct_of_T_cells: 0.582\n",
      "      CD45ROposCD183negCD196neg_pct_of_CD3posCD4pos: 10.2\n",
      "      CD45ROposCD183negCD196pos_pct_of_CD3posCD4pos: 9.58\n",
      "      CD45ROposCD183posCD196neg_pct_of_CD3posCD4pos: 19.7\n",
      "      CD45ROposCD183posCD196pos_pct_of_CD3posCD4pos: 19.8\n",
      "      HLADRpos_pct_of_CD4posCD45ROposCD183negCD196neg: 6.8\n",
      "      HLADRposCD38pos_pct_of_CD4posCD45ROposCD183negCD196neg: 1.36\n",
      "      HLADRpos_pct_of_CD4posCD45ROposCD183negCD196pos: 11.2\n",
      "      HLADRposCD38pos_pct_of_CD4posCD45ROposCD183negCD196pos: 2.17\n",
      "      HLADRpos_pct_of_CD4posCD45ROposCD183posCD196neg: 9.67\n",
      "      HLADRposCD38pos_pct_of_CD4posCD45ROposCD183posCD196neg: 4.75\n",
      "      HLADRpos_pct_of_CD4posCD45ROposCD183posCD196pos: 3.5\n",
      "      HLADRposCD38pos_pct_of_CD4posCD45ROposCD183posCD196pos: 1.22\n",
      "      CD183loCD196pos_pct_of_CD3posCD4neg: 20.9\n",
      "      CD183posCD196neg_pct_of_CD3posCD4neg: 37.3\n",
      "      CD183negCD196neg_pct_of_CD3posCD4neg: 8.5\n",
      "      HLApos_pct_of_CD4negCD45ROposCD183loCD196pos: 2.6\n",
      "      HLAposCD38pos_pct_of_CD4negCD45ROposCD183loCD196pos: 0.651\n",
      "      HLApos_pct_of_CD4negCD45ROposCD183posCD196neg: 23.3\n",
      "      HLAposCD38pos_pct_of_CD4negCD45ROposCD183posCD196neg: 7.76\n",
      "      HLApos_pct_of_CD4negCD45ROposCD183negCD196neg: 7.98\n",
      "      HLAposCD38pos_pct_of_CD4negCD45ROposCD183negCD196neg: 2.13\n",
      "      CD183neg_pct_of_CD3posCD4negCD45ROneg: 26.6\n",
      "      CD183lo_pct_of_CD3posCD4negCD45ROneg: 57.5\n",
      "      CD183hi_pct_of_CD3posCD4negCD45ROneg: 15.9\n",
      "      Tfh_pct_of_CD4: 3.29\n",
      "      CD183pos_pct_of_Tfh: 80.2\n",
      "      Precursor_Tfh_pct_of_CD4: 0.335\n",
      "      Memory_Tfh_pct_of_CD4: 0.651\n",
      "      Tfh_pct_of_CD4posCD45RAnegCD185pos: 23.5\n",
      "      Precursor_Tfh_pct_of_CD4posCD45RAnegCD185pos: 2.39\n",
      "      Memory_Tfh__pct_of_CD4posCD45RAnegCD185pos: 4.63\n",
      "      Naive_CD4_CD279mfi: 173.3\n",
      "      Tem_CD4_CD279mfi: 241.0\n",
      "      Tcm_CD4_CD279mfi: 213.2\n",
      "      Temra_CD4_CD279mfi: 174.7\n",
      "      Temra_CD4_CD279mfi_Rel_Diff: 0.807847663\n",
      "      Tcm_CD4_CD279mfi_Rel_Diff: 23.0236584\n",
      "      Tem_CD4_CD279mfi_Rel_Diff: 39.06520485\n",
      "      CD279pos_pct_of_Naive_CD4: 1.61\n",
      "      CD279pos_pct_of_CD4_Tem: 15.6\n",
      "      CD279pos_pct_of_CD4_Tcm: 10.5\n",
      "      CD279pos_pct_of_CD4_Temra: 10.0\n",
      "      CD183pos_pct_of_Naive_CD4: 10.3\n",
      "      CD183pos_pct_of_CD4_Tem: 79.6\n",
      "      CD183pos_pct_of_CD4_Tcm: 65.0\n",
      "      CD183pos_pct_of_CD4_Temra: 80.0\n",
      "      CD185pos_pct_of_Naive_CD4: 13.4\n",
      "      CD185pos_pct_of_CD4_Tem: 2.61\n",
      "      CD185pos_pct_of_CD4_Tcm: 31.9\n",
      "      CD185pos_pct_of_CD4_Temra: 3.33\n",
      "      Naive_CD8_CD279mfi: 144.3\n",
      "      Tem_CD8_CD279mfi: 268.0\n",
      "      Tcm_CD8_CD279mfi: 223.0\n",
      "      Temra_CD8_CD279mfi: 186.4\n",
      "      Temra_CD8_CD279mfi_Rel_Diff: 29.17532918\n",
      "      Tcm_CD8_CD279mfi_Rel_Diff: 54.53915454\n",
      "      Tem_CD8_CD279mfi_Rel_Diff: 85.72418572\n",
      "      CD279pos_pct_of_Naive_CD8: 3.23\n",
      "      CD279pos_pct_of_CD8_Tcm: 13.1\n",
      "      CD279pos_pct_of_CD8_Tem: 23.3\n",
      "      CD279pos_pct_of_CD8_Temra: 16.5\n",
      "      CD183pos_pct_of_Naive_CD8: 61.3\n",
      "      CD183pos_pct_of_CD8_Tcm: 78.7\n",
      "      CD183pos_pct_of_CD8_Tem: 62.7\n",
      "      CD183pos_pct_of_CD8_Temra: 51.4\n",
      "      CD185pos_pct_of_Naive_CD8: 11.6\n",
      "      CD185pos_pct_of_CD8_Tcm: 6.0\n",
      "      CD185pos_pct_of_CD8_Tem: 2.08\n",
      "      CD185pos_pct_of_CD8_Temra: 3.14\n",
      "      CD183neg_pctof_CD3posCD4neg_Naive: 28.1\n",
      "      CD183hi_pct_of_CD3posCD4neg_Naive: 4.84\n",
      "      CD183lo_pct_of_CD3posCD4neg_Naive: 67.1\n",
      "      CD183neg_pct_of_CD3posCD4neg_Tem: 24.1\n",
      "      CD183hi_pct_of_CD3posCD4neg_Tem: 1.54\n",
      "      CD183lo_pct_of_CD3posCD4neg_Tem: 74.4\n",
      "      CD183neg_pct_of_CD3posCD4neg_Tcm: 16.5\n",
      "      CD183hi_pct_of_CD3posCD4neg_Tcm: 29.8\n",
      "      CD183lo_pct_of_CD3posCD4neg_Tcm: 53.7\n",
      "      CD183neg_pct_of_CD3posCD4neg_Temra: 32.5\n",
      "      CD183hi_pct_of_CD3posCD4neg_Temra: 1.18\n",
      "      CD183lo_pct_of_CD3posCD4neg_Temra: 66.3\n",
      "      Treg_pct_of_CD4pos: 5.42\n",
      "      CD45ROpos_pct_of_Treg: 72.3\n",
      "      CD25mfi_of_CD45ROneg_Treg: 468.8\n",
      "      CD25mfi_of_CD45ROpos_Treg: 523.8\n",
      "      CD45ROpos_Treg_CD25mfi_Rel_Diff: 11.73208191\n",
      "      CD45ROposHLADRposCCR4pos_pct_of_Treg: 22.0\n",
      "      CD25mfi_of_CD45ROneg_Tconv: 37.9\n",
      "      CD25mfi_of_CD45ROpos_Tconv: 95.9\n",
      "      CD45ROpos_Tconv_CD25mfi_Rel_Diff: 153.0343008\n",
      "      CD45ROposCCR4pos_pct_of_Tconv: 34.6\n",
      "      CD45ROposHLADRposCCR4pos_pct_of_Tconv: 3.54\n",
      "      CD25mfi_of_CD45ROnegCD3posCD4neg: 42.1\n",
      "      CD25mfi_of_CD45ROposCD3posCD4neg: 76.9\n",
      "      CD45ROposCD3posCD4neg_CD25mfi_Rel_Diff: 82.66033254\n",
      "      CD45ROposCCR4pos_pct_of_CD3posCD4neg: 19.5\n",
      "      CD45ROposHLADRposCCR4pos_pct_of_CD3posCD4neg: 3.83\n",
      "      Wbc_(10^3/¬µl): 6.7\n",
      "      Rbc_(10^6/¬µl): 4.84\n",
      "      Hgb_(g/dl): 14.9\n",
      "      Hct_(pct): 45.1\n",
      "      Mcv_(fl): 93.0\n",
      "      Mch_(pg): 30.8\n",
      "      Mchc_(g/dl): 33.1\n",
      "      Rdw_(pct): 10.7\n",
      "      Plt_(10^3/¬µl): 197.0\n",
      "      Mpv_(fl): 9.5\n",
      "      Ne_(pct): 66.2\n",
      "      Ly_(pct): 23.7\n",
      "      Mo_(pct): 7.9\n",
      "      Eo_(pct): 2.5\n",
      "      Ba_(pct): 0.4\n",
      "      Ne#_(10^3/¬µl): 4.44\n",
      "      Ly#_(10^3/¬µl): 1.54\n",
      "      Mo#_(10^3/¬µl): 0.53\n",
      "      Eo#_(10^3/¬µl): 0.17\n",
      "      Ba#_(10^3/¬µl): 0.03\n",
      "- Notes: This record has a Diabetes_Status of T1D and an age of 43.13483915. It includes various percentages related to cell types and markers, as well as complete blood count information.\n",
      "\n",
      "CSV Record Summary:\n",
      "- Dataset: 685000ea12c7b37a8d058b9c (ID: 685056e712c7b37a8d058d02)\n",
      "- Columns: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome, dataset_id\n",
      "- Record:\n",
      "      Pregnancies: 2.0\n",
      "      Glucose: 105.0\n",
      "      BloodPressure: 75.0\n",
      "      SkinThickness: 0.0\n",
      "      Insulin: 0.0\n",
      "      BMI: 23.3\n",
      "      DiabetesPedigreeFunction: 0.56\n",
      "      Age: 53.0\n",
      "      Outcome: 0.0\n",
      "- Notes: This record contains information about pregnancies, glucose, blood pressure, skin thickness, insulin, BMI, diabetes pedigree function, age, and outcome.\n",
      "\n",
      "Differences:\n",
      "\n",
      "- The datasets are different. The first record comes from dataset '685000ea12c7b37a8d058b9c' which contains detailed information about cell populations and blood counts related to diabetes, while the second record comes from a dataset with ID '685052b612c7b37a8d059b93' which contains more general diabetes-related information.\n",
      "- The first record has a Diabetes_Status of 'T1D', while this column does not exist in the second record.\n",
      "- The first record contains detailed percentages of various cell types (B cells, T cells, NK cells, etc.) and their markers, while the second record does not contain any of this information.\n",
      "- The first record contains complete blood count information (WBC, RBC, HGB, HCT, etc.), while the second record does not contain this information.\n",
      "- The second record contains information about Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, and DiabetesPedigreeFunction, which are not present in the first record.\n",
      "- The ages are different (43.13483915 vs 53.0)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/f12_3w995dg5jn6dh4r5vvxm0000gn/T/ipykernel_50477/1548652449.py:11: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚ö†Ô∏è  Empty input skipped.\n",
      "‚ö†Ô∏è  Empty input skipped.\n",
      "‚ö†Ô∏è  Empty input skipped.\n",
      "‚ö†Ô∏è  Empty input skipped.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter notebooks and IPython environments\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "await chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
